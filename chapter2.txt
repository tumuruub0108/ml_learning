# Chapter 2

PIPELINES
A sequence of data processing components is called a data pipeline.Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.
Өгөгдөл боловсруулах дараалсан бүрдлүүдийг “data pipeline" гэж нэрлэдэг.сМашин сургалтын системд ийм pipeline маш түгээмэл байдаг. Учир нь их хэмжээний өгөгдлийг боловсруулах, олон төрлийн хувиргалт хийх шаардлагатай байдаг.

Pipeline гэдэг нь өгөгдлийг дараалсан алхамуудаар боловсруулах явц.

    Жишээ нь:
        Түүхий өгөгдөл цуглуулах
        Өгөгдөл цэвэрлэх
        Шинж чанар (features) гаргаж авах
    Загварт оруулах:
         Энэ бүхийг нэг урсгал (pipeline) болгох нь автоматжуулж, алдаа багасгаж, дахин ашиглахад хялбар болгодог.

Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output.


Synchronous (синхрон) → нэг бүрдэл дуусахад дараагийнх эхэлдэг.
Asynchronous (асинхрон) → бүрдлүүдийг заавал хүлээлгүй, зэрэгцээд эсвэл бие даан ажиллаж болно.

    Жишээ нь, data pipeline дотор:
        Өгөгдөл уншиж буй бүрдэл
        Өгөгдөл цэвэрлэж буй бүрдэл
        Загварт оруулж буй бүрдэл
        
Эдгээр нь асинхрон ажиллавал, нэг нь дуусахаар нөгөө нь эхлэхийг хүлээхгүй, өгөгдөл ирэхтэй зэрэгцээд шууд ажиллаж чадна. Энэ нь хурд, үр ашгийг нэмэгдүүлдэг.


Correlation is a statistical measure that shows how strongly two variables are related to each other.
If two things change together in a predictable way, they are said to be correlated.

The most common measure is the correlation coefficient (often denoted as r), which ranges from -1 to +1:
    +1 → perfect positive correlation (when one increases, the other always increases).
    0 → no correlation (no relationship between them).
    -1 → perfect negative correlation (when one increases, the other always decreases)

Example:
    Positive correlation: The number of hours studied and exam score → usually, more study hours means higher scores.
    Negative correlation: The speed of a car and the time it takes to reach a destination → higher speed means less time.
    No correlation: Shoe size and intelligence → unrelated.


# You create these new attributes as follows:
    df_house["rooms_per_house"] = df_house["total_rooms"] / df_house["households"]
    df_house["bedrooms_ratio"] = df_house["total_bedrooms"] / df_house["total_rooms"]
    df_house["people_per_house"] = df_house["population"] / df_house["households"]


# Clean the Data
inplace=True
    Modify the original DataFrame directly.
    If inplace=False (default), it would return a new DataFrame without changing original DataFrame.

option 1: dropna
    Дутуу утга бүхий мөрүүдийг устгана
    housing.dropna(subset=["total_bedrooms"], inplace=True) 

    Use when:
        Only a few rows are missing data.
        The missing column/row is not critical or you have enough data left.
        You want to remove NaNs specifically.

    ✅ Pros: Simple, removes incomplete data.
    ❌ Cons: Can reduce dataset size too much if missing values are frequent.

option 2: drop
    Баганыг бүхэлд нь устгана
    housing.drop("total_bedrooms", axis=1)

    Use when:
        Columns or rows are irrelevant, not missing.
        Example: ID columns, text columns that won’t be used in ML.

    ✅ Pros: Removes irrelevant features, keeps dataset clean.
    ❌ Cons: Doesn’t handle missing values unless combined with dropna().

option 3: fillna
    Fill missing values with the median
    median = housing["total_bedrooms"].median() 
    housing["total_bedrooms"].fillna(median, inplace=True)

    Use when:
        Column is important for analysis/model.
        You cannot afford to lose rows.
        Impute missing values with a statistic or method.

option 4 Scikit-Learn class: SimpleImputer
    How it works
    When you create a SimpleImputer, you define a strategy:
        "mean" → replaces missing values with the mean of the column (numeric data).
        "median" → replaces missing values with the median of the column.
        "most_frequent" → replaces with the most common value (works for both numeric & categorical).
        "constant" → replaces with a specific value you define (e.g., 0, "missing", etc.).

    Benefits of SimpleImputer
        Prevents data loss → Instead of dropping rows/columns with missing values, you fill them smartly.
        Keeps dataset usable → Especially important when missing values are common.
        Different strategies → Flexible for both numerical (mean, median) and categorical (most_frequent, constant) data.
        Pipeline integration → Works smoothly with Scikit-Learn pipelines, so preprocessing and modeling stay consistent.

imputer = SimpleImputer(strategy="median")
housing_num = housing.select_dtypes(include=[np.number])
imputer.fit(housing_num)

Now you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:
X = imputer.transform(housing_num)

output of imputer.transform(housing_num) is a NumPy array: X has neither column names nor index. Luckily, it’s not too hard to wrap X in a DataFrame and recover the column names and index from housing_num:
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
index=housing_num.index)

TIP
There are also more powerful imputers available in the sklearn.impute package (both fornumerical features only):
KNNImputer replaces each missing value with the mean of the k-nearest neighbors’values for that feature. The distance is based on all the available features.


# Handling Text and Categorical Attributes  
Most machine learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers

# Option 1 Scikit-Learn’s OrdinalEncoder class
converts categorical values (strings/labels) into integer codes.

data = pd.DataFrame({
    "city": ["Seoul", "Busan", "Daegu", "Seoul"]
})
encoder = OrdinalEncoder()
data["city_encoded"] = encoder.fit_transform(data[["city"]])\

✅ Benefits:
    Very simple, compact representation (just one column).
    Works well when categories have a natural order (e.g., "low" < "medium" < "high").
    Saves memory compared to one-hot.

⚠️ Warning: If categories have no natural order (like city names), models may incorrectly assume that Seoul (2.0) > Busan (0.0) in a meaningful way.

# option 2 Scikit-Learn’s OneHotEncoder class
OneHotEncoder creates a binary column (0/1) for each category.
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(data[["city"]])
✔ It creates 3 new columns: Busan, Daegu, Seoul, with 1 marking the active category.

✅ Benefits:
    No false ordering → each category is independent.
    Better for ML models (like linear regression, logistic regression) that assume numeric distance matters.
    Widely used when categories are nominal (no natural order).

⚠️ Downside: Increases dataset size (high-dimensional if many categories).

| Feature        | OrdinalEncoder                       | OneHotEncoder                      |
| -------------- | ------------------------------------ | ---------------------------------- |
| **Output**     | Single integer per category          | Multiple binary columns            |
| **Best for**   | Ordered categories (low/medium/high) | Unordered categories (city, color) |
| **Memory**     | Efficient (1 column)                 | More memory (n columns)            |
| **Model risk** | May mislead model if no real order   | Safe, no false order               |


# Feature Scaling and Transformation
# Feature Scaling
the numerical features in your dataset have been transformed to be on a similar scale.
Feature Scaling гэдэг нь өгөгдлийн багануудын утгуудыг нэг хэмжээс рүү оруулах үйл явц юм. Учир нь зарим машин сургалтын алгоритмууд (gradient descent, k-NN, SVM, neural networks) өгөгдлийн утгын хэмжээсээс хамаарч ажилладаг.

machine learning algorithms don’t perform well when the input numerical attributes have very different scales.
    Жишээ:
        Байшингийн хэмжээ (square feet): 0 – 5000
        Унтлагын өрөөний тоо: 0 – 5
Purpose: Prevent features with large values (e.g., income in thousands) from dominating smaller features (e.g., age).
There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.
    Ерөнхий арга:
        Min-Max(Normalization) Scaling (values between 0 and 1)
        Standardization Баганыг mean = 0, std = 1 болгоно.
        Max Abs Scaling (Sparse өгөгдөлд тохиромжтой)
    Баганын утгыг [-1,1] хүртэл жигдлэнэ.

    Min-max scaling:
        from sklearn.preprocessing import MinMaxScaler
        min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
        housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
    Standardization scaling:
        std_scaler = StandardScaler()
        housing_num_std_scaled = std_scaler.fit_transform(housing_num)


# Feature Transformation
Feature transformation is the process of changing the representation of data features to make them more suitable for machine learning models.

It doesn’t create new information but reshapes existing features to:
    Make patterns easier to learn
    Improve model accuracy
    Speed up training

Why do we need it?
    Raw data is often messy: different scales, distributions, and types of features.
    Transforming features can:
        Handle skewed data
        Reduce the effect of outliers
        Improve convergence of algorithms (especially gradient-based)
        Make relationships more linear
        Prepare data for specific models

🔹 Common Types of Feature Transformations
    1. Scaling
        Rescale features so they have similar ranges.
        Min-Max Scaling → transforms values into [0,1].
        Standardization (Z-score) → mean = 0, std = 1.
    2. Normalization
        Transform each sample to have unit length (used in text/embedding data).
        Good for models using cosine similarity.
    3. Log / Power Transformation
        Fix skewed distributions (long tail values).
        Example: income, population.
            Income = [10, 100, 10000] → log(Income) = [1, 2, 4]
    4. Bucketizing / Binning
        Convert continuous values into discrete intervals (e.g., age groups, income ranges).
    5. Encoding Categorical Data
        OrdinalEncoder → maps categories to numbers with order.
        OneHotEncoder → creates binary columns per category.
    6. Polynomial / Interaction Features
        Create new features by combining existing ones.
        📌 Example: x1 * x2, x1^2 → helps linear models capture non-linear patterns.
    7. Dimensionality Reduction
        Transforms high-dimensional features into fewer dimensions (while keeping most info).
            PCA (Principal Component Analysis)
            t-SNE / UMAP (for visualization)
    
💡 Дүгнэлт:
    Scaling → Алгоритмуудыг зөв ажиллуулахад чухал
    Transformation → model гүйцэтгэлийг сайжруулахад ашиглагддаг

the values are shifted
1️⃣ Mean shift (дундаж шилжүүлэлт)
Мэдээллийн багануудын дундаж (mean) нь 0-д ойр байхаар shift хийгддэг.
    Жишээ:
        Original feature: [2, 4, 6, 8]
    Mean = 5
        Shifted (centered) = [2-5, 4-5, 6-5, 8-5] = [-3, -1, 1, 3]  
Үүнийг Mean normalization гэж нэрлэдэг.
    Учир нь:
        ML алгоритмууд (жишээ нь: gradient descent, PCA) нь centered data-д хурдан ажилладаг.


# Custom Transformers
    In Scikit-Learn, a Transformer is an object that:
        Learns something from data using .fit() (e.g., mean, scaling parameters).
        Applies a transformation to the data using .transform().
        Can do both in one step with .fit_transform().

    Examples of built-in transformers:
        StandardScaler → standardizes features.
        SimpleImputer → fills missing values.
        OneHotEncoder → encodes categorical features.

They are the building blocks of preprocessing pipelines.

# Why Write Custom Transformers?
Real datasets are messy. Pre-built tools don’t always fit your domain-specific needs.
That’s why you might need custom transformers.

    ✅ Reasons:
        Custom cleanup operations
            Remove unwanted symbols (e.g., $ in price).
            Fix invalid values (negative ages).

        Custom feature engineering
            Derive new features (e.g., rooms_per_house = total_rooms / households).
            Group rare categories into “Other.”

        Combining specific attributes
            Interaction features (e.g., income × education).
            Aggregate features (e.g., population per household).

        Reusable in Pipelines
            Once written, your custom transformer can be dropped into a Scikit-Learn Pipeline or ColumnTransformer.
            Keeps preprocessing and training consistent.

# Transformation Pipelines


# Select a Performance Measure
Your next step is to select a performance measure. A typical performance measure for regression problems is the root mean square error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.


RMSE = Root Mean Square Error
Энэ нь моделийн таамаглал ба бодит утгын зөрүүг хэмжих хамгийн түгээмэл арга юм.
    Таамаглал (predicted value) ба бодит утга (actual value) хоорондын зөрүү (error) олно.
    Тэр зөрүүг квадрат болгоно (ингэснээр сөрөг утгагүй болно).
    Бүх алдаануудын квадратын дундаж (mean) авна.
    Эцэст нь дундаж квадратыг дахин үржвэр (root) авна.

    Онцлог:
        RMSE бага байх тусам таамаглал бодит утгад ойр байна гэсэн үг.
        RMSE нь анхны утгын нэгжтэй ижил байдаг (жишээ нь, хэрвээ үнэ төгрөгөөр бол RMSE мөн төгрөгөөр хэмжигдэнэ).