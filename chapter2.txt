# Chapter 2

PIPELINES
A sequence of data processing components is called a data pipeline.Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.
”®–≥”©–≥–¥”©–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ ‚Äúdata pipeline" –≥—ç–∂ –Ω—ç—Ä–ª—ç–¥—ç–≥.—Å–ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Å–∏—Å—Ç–µ–º–¥ –∏–π–º pipeline –º–∞—à —Ç“Ø–≥—ç—ç–º—ç–ª –±–∞–π–¥–∞–≥. –£—á–∏—Ä –Ω—å –∏—Ö —Ö—ç–º–∂—ç—ç–Ω–∏–π ”©–≥”©–≥–¥–ª–∏–π–≥ –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö, –æ–ª–æ–Ω —Ç”©—Ä–ª–∏–π–Ω —Ö—É–≤–∏—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–∞–π–¥–∞–≥.

Pipeline –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–≥ –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –∞–ª—Ö–∞–º—É—É–¥–∞–∞—Ä –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö —è–≤—Ü.

    –ñ–∏—à—ç—ç –Ω—å:
        –¢“Ø“Ø—Ö–∏–π ”©–≥”©–≥–¥”©–ª —Ü—É–≥–ª—É—É–ª–∞—Ö
        ”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç—Ö
        –®–∏–Ω–∂ —á–∞–Ω–∞—Ä (features) –≥–∞—Ä–≥–∞–∂ –∞–≤–∞—Ö
    –ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∞—Ö:
         –≠–Ω—ç –±“Ø—Ö–∏–π–≥ –Ω—ç–≥ —É—Ä—Å–≥–∞–ª (pipeline) –±–æ–ª–≥–æ—Ö –Ω—å –∞–≤—Ç–æ–º–∞—Ç–∂—É—É–ª–∂, –∞–ª–¥–∞–∞ –±–∞–≥–∞—Å–≥–∞–∂, –¥–∞—Ö–∏–Ω –∞—à–∏–≥–ª–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä –±–æ–ª–≥–æ–¥–æ–≥.

Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output.


Synchronous (—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –Ω—ç–≥ –±“Ø—Ä–¥—ç–ª –¥—É—É—Å–∞—Ö–∞–¥ –¥–∞—Ä–∞–∞–≥–∏–π–Ω—Ö —ç—Ö—ç–ª–¥—ç–≥.
Asynchronous (–∞—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ –∑–∞–∞–≤–∞–ª —Ö“Ø–ª—ç—ç–ª–≥“Ø–π, –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —ç—Å–≤—ç–ª –±–∏–µ –¥–∞–∞–Ω –∞–∂–∏–ª–ª–∞–∂ –±–æ–ª–Ω–æ.

    –ñ–∏—à—ç—ç –Ω—å, data pipeline –¥–æ—Ç–æ—Ä:
        ”®–≥”©–≥–¥”©–ª —É–Ω—à–∏–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        ”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        –ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        
–≠–¥–≥—ç—ç—Ä –Ω—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω –∞–∂–∏–ª–ª–∞–≤–∞–ª, –Ω—ç–≥ –Ω—å –¥—É—É—Å–∞—Ö–∞–∞—Ä –Ω”©–≥”©”© –Ω—å —ç—Ö–ª—ç—Ö–∏–π–≥ —Ö“Ø–ª—ç—ç—Ö–≥“Ø–π, ”©–≥”©–≥–¥”©–ª –∏—Ä—ç—Ö—Ç—ç–π –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —à—É—É–¥ –∞–∂–∏–ª–ª–∞–∂ —á–∞–¥–Ω–∞. –≠–Ω—ç –Ω—å —Ö—É—Ä–¥, “Ø—Ä –∞—à–≥–∏–π–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª–¥—ç–≥.


Correlation is a statistical measure that shows how strongly two variables are related to each other.
If two things change together in a predictable way, they are said to be correlated.

The most common measure is the correlation coefficient (often denoted as r), which ranges from -1 to +1:
    +1 ‚Üí perfect positive correlation (when one increases, the other always increases).
    0 ‚Üí no correlation (no relationship between them).
    -1 ‚Üí perfect negative correlation (when one increases, the other always decreases)

Example:
    Positive correlation: The number of hours studied and exam score ‚Üí usually, more study hours means higher scores.
    Negative correlation: The speed of a car and the time it takes to reach a destination ‚Üí higher speed means less time.
    No correlation: Shoe size and intelligence ‚Üí unrelated.


# You create these new attributes as follows:
    df_house["rooms_per_house"] = df_house["total_rooms"] / df_house["households"]
    df_house["bedrooms_ratio"] = df_house["total_bedrooms"] / df_house["total_rooms"]
    df_house["people_per_house"] = df_house["population"] / df_house["households"]


# Clean the Data
inplace=True
    Modify the original DataFrame directly.
    If inplace=False (default), it would return a new DataFrame without changing original DataFrame.

option 1: dropna
    –î—É—Ç—É—É —É—Ç–≥–∞ –±“Ø—Ö–∏–π –º”©—Ä“Ø“Ø–¥–∏–π–≥ —É—Å—Ç–≥–∞–Ω–∞
    housing.dropna(subset=["total_bedrooms"], inplace=True) 

    Use when:
        Only a few rows are missing data.
        The missing column/row is not critical or you have enough data left.
        You want to remove NaNs specifically.

    ‚úÖ Pros: Simple, removes incomplete data.
    ‚ùå Cons: Can reduce dataset size too much if missing values are frequent.

option 2: drop
    –ë–∞–≥–∞–Ω—ã–≥ –±“Ø—Ö—ç–ª–¥ –Ω—å —É—Å—Ç–≥–∞–Ω–∞
    housing.drop("total_bedrooms", axis=1)

    Use when:
        Columns or rows are irrelevant, not missing.
        Example: ID columns, text columns that won‚Äôt be used in ML.

    ‚úÖ Pros: Removes irrelevant features, keeps dataset clean.
    ‚ùå Cons: Doesn‚Äôt handle missing values unless combined with dropna().

option 3: fillna
    Fill missing values with the median
    median = housing["total_bedrooms"].median() 
    housing["total_bedrooms"].fillna(median, inplace=True)

    Use when:
        Column is important for analysis/model.
        You cannot afford to lose rows.
        Impute missing values with a statistic or method.

option 4 Scikit-Learn class: SimpleImputer
    How it works
    When you create a SimpleImputer, you define a strategy:
        "mean" ‚Üí replaces missing values with the mean of the column (numeric data).
        "median" ‚Üí replaces missing values with the median of the column.
        "most_frequent" ‚Üí replaces with the most common value (works for both numeric & categorical).
        "constant" ‚Üí replaces with a specific value you define (e.g., 0, "missing", etc.).

    Benefits of SimpleImputer
        Prevents data loss ‚Üí Instead of dropping rows/columns with missing values, you fill them smartly.
        Keeps dataset usable ‚Üí Especially important when missing values are common.
        Different strategies ‚Üí Flexible for both numerical (mean, median) and categorical (most_frequent, constant) data.
        Pipeline integration ‚Üí Works smoothly with Scikit-Learn pipelines, so preprocessing and modeling stay consistent.

imputer = SimpleImputer(strategy="median")
housing_num = housing.select_dtypes(include=[np.number])
imputer.fit(housing_num)

Now you can use this ‚Äútrained‚Äù imputer to transform the training set by replacing missing values with the learned medians:
X = imputer.transform(housing_num)

output of imputer.transform(housing_num) is a NumPy array: X has neither column names nor index. Luckily, it‚Äôs not too hard to wrap X in a DataFrame and recover the column names and index from housing_num:
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
index=housing_num.index)

TIP
There are also more powerful imputers available in the sklearn.impute package (both fornumerical features only):
KNNImputer replaces each missing value with the mean of the k-nearest neighbors‚Äôvalues for that feature. The distance is based on all the available features.


# Handling Text and Categorical Attributes  
Most machine learning algorithms prefer to work with numbers, so let‚Äôs convert these categories from text to numbers

# Option 1 Scikit-Learn‚Äôs OrdinalEncoder class
converts categorical values (strings/labels) into integer codes.

data = pd.DataFrame({
    "city": ["Seoul", "Busan", "Daegu", "Seoul"]
})
encoder = OrdinalEncoder()
data["city_encoded"] = encoder.fit_transform(data[["city"]])\

‚úÖ Benefits:
    Very simple, compact representation (just one column).
    Works well when categories have a natural order (e.g., "low" < "medium" < "high").
    Saves memory compared to one-hot.

‚ö†Ô∏è Warning: If categories have no natural order (like city names), models may incorrectly assume that Seoul (2.0) > Busan (0.0) in a meaningful way.

# option 2 Scikit-Learn‚Äôs OneHotEncoder class
OneHotEncoder creates a binary column (0/1) for each category.
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(data[["city"]])
‚úî It creates 3 new columns: Busan, Daegu, Seoul, with 1 marking the active category.

‚úÖ Benefits:
    No false ordering ‚Üí each category is independent.
    Better for ML models (like linear regression, logistic regression) that assume numeric distance matters.
    Widely used when categories are nominal (no natural order).

‚ö†Ô∏è Downside: Increases dataset size (high-dimensional if many categories).

| Feature        | OrdinalEncoder                       | OneHotEncoder                      |
| -------------- | ------------------------------------ | ---------------------------------- |
| **Output**     | Single integer per category          | Multiple binary columns            |
| **Best for**   | Ordered categories (low/medium/high) | Unordered categories (city, color) |
| **Memory**     | Efficient (1 column)                 | More memory (n columns)            |
| **Model risk** | May mislead model if no real order   | Safe, no false order               |


# Feature Scaling and Transformation
# Feature Scaling
the numerical features in your dataset have been transformed to be on a similar scale.
Feature Scaling –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥—ã–Ω —É—Ç–≥—É—É–¥—ã–≥ –Ω—ç–≥ —Ö—ç–º–∂—ç—ç—Å —Ä“Ø“Ø –æ—Ä—É—É–ª–∞—Ö “Ø–π–ª —è–≤—Ü —é–º. –£—á–∏—Ä –Ω—å –∑–∞—Ä–∏–º –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º—É—É–¥ (gradient descent, k-NN, SVM, neural networks) ”©–≥”©–≥–¥–ª–∏–π–Ω —É—Ç–≥—ã–Ω —Ö—ç–º–∂—ç—ç—Å—ç—ç—Å —Ö–∞–º–∞–∞—Ä—á –∞–∂–∏–ª–ª–∞–¥–∞–≥.

machine learning algorithms don‚Äôt perform well when the input numerical attributes have very different scales.
    –ñ–∏—à—ç—ç:
        –ë–∞–π—à–∏–Ω–≥–∏–π–Ω —Ö—ç–º–∂—ç—ç (square feet): 0 ‚Äì 5000
        –£–Ω—Ç–ª–∞–≥—ã–Ω ”©—Ä”©”©–Ω–∏–π —Ç–æ–æ: 0 ‚Äì 5
Purpose: Prevent features with large values (e.g., income in thousands) from dominating smaller features (e.g., age).
There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.
    –ï—Ä”©–Ω—Ö–∏–π –∞—Ä–≥–∞:
        Min-Max(Normalization) Scaling (values between 0 and 1)
        Standardization –ë–∞–≥–∞–Ω—ã–≥ mean = 0, std = 1 –±–æ–ª–≥–æ–Ω–æ.
        Max Abs Scaling (Sparse ”©–≥”©–≥–¥”©–ª–¥ —Ç–æ—Ö–∏—Ä–æ–º–∂—Ç–æ–π)
    –ë–∞–≥–∞–Ω—ã–Ω —É—Ç–≥—ã–≥ [-1,1] —Ö“Ø—Ä—Ç—ç–ª –∂–∏–≥–¥–ª—ç–Ω—ç.

    Min-max scaling:
        from sklearn.preprocessing import MinMaxScaler
        min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
        housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
    Standardization scaling:
        std_scaler = StandardScaler()
        housing_num_std_scaled = std_scaler.fit_transform(housing_num)


# Feature Transformation
Feature transformation is the process of changing the representation of data features to make them more suitable for machine learning models.

It doesn‚Äôt create new information but reshapes existing features to:
    Make patterns easier to learn
    Improve model accuracy
    Speed up training

Why do we need it?
    Raw data is often messy: different scales, distributions, and types of features.
    Transforming features can:
        Handle skewed data
        Reduce the effect of outliers
        Improve convergence of algorithms (especially gradient-based)
        Make relationships more linear
        Prepare data for specific models

üîπ Common Types of Feature Transformations
    1. Scaling
        Rescale features so they have similar ranges.
        Min-Max Scaling ‚Üí transforms values into [0,1].
        Standardization (Z-score) ‚Üí mean = 0, std = 1.
    2. Normalization
        Transform each sample to have unit length (used in text/embedding data).
        Good for models using cosine similarity.
    3. Log / Power Transformation
        Fix skewed distributions (long tail values).
        Example: income, population.
            Income = [10, 100, 10000] ‚Üí log(Income) = [1, 2, 4]
    4. Bucketizing / Binning
        Convert continuous values into discrete intervals (e.g., age groups, income ranges).
    5. Encoding Categorical Data
        OrdinalEncoder ‚Üí maps categories to numbers with order.
        OneHotEncoder ‚Üí creates binary columns per category.
    6. Polynomial / Interaction Features
        Create new features by combining existing ones.
        üìå Example: x1 * x2, x1^2 ‚Üí helps linear models capture non-linear patterns.
    7. Dimensionality Reduction
        Transforms high-dimensional features into fewer dimensions (while keeping most info).
            PCA (Principal Component Analysis)
            t-SNE / UMAP (for visualization)
    
üí° –î“Ø–≥–Ω—ç–ª—Ç:
    Scaling ‚Üí –ê–ª–≥–æ—Ä–∏—Ç–º—É—É–¥—ã–≥ –∑”©–≤ –∞–∂–∏–ª–ª—É—É–ª–∞—Ö–∞–¥ —á—É—Ö–∞–ª
    Transformation ‚Üí model –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–≥–¥–¥–∞–≥

the values are shifted
1Ô∏è‚É£ Mean shift (–¥—É–Ω–¥–∞–∂ —à–∏–ª–∂“Ø“Ø–ª—ç–ª—Ç)
–ú—ç–¥—ç—ç–ª–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥—ã–Ω –¥—É–Ω–¥–∞–∂ (mean) –Ω—å 0-–¥ –æ–π—Ä –±–∞–π—Ö–∞–∞—Ä shift —Ö–∏–π–≥–¥–¥—ç–≥.
    –ñ–∏—à—ç—ç:
        Original feature: [2, 4, 6, 8]
    Mean = 5
        Shifted (centered) = [2-5, 4-5, 6-5, 8-5] = [-3, -1, 1, 3]  
“Æ“Ø–Ω–∏–π–≥ Mean normalization –≥—ç–∂ –Ω—ç—Ä–ª—ç–¥—ç–≥.
    –£—á–∏—Ä –Ω—å:
        ML –∞–ª–≥–æ—Ä–∏—Ç–º—É—É–¥ (–∂–∏—à—ç—ç –Ω—å: gradient descent, PCA) –Ω—å centered data-–¥ —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª–∞–¥–∞–≥.


# Custom Transformers
    In Scikit-Learn, a Transformer is an object that:
        Learns something from data using .fit() (e.g., mean, scaling parameters).
        Applies a transformation to the data using .transform().
        Can do both in one step with .fit_transform().

    Examples of built-in transformers:
        StandardScaler ‚Üí standardizes features.
        SimpleImputer ‚Üí fills missing values.
        OneHotEncoder ‚Üí encodes categorical features.

They are the building blocks of preprocessing pipelines.

# Why Write Custom Transformers?
Real datasets are messy. Pre-built tools don‚Äôt always fit your domain-specific needs.
That‚Äôs why you might need custom transformers.

    ‚úÖ Reasons:
        Custom cleanup operations
            Remove unwanted symbols (e.g., $ in price).
            Fix invalid values (negative ages).

        Custom feature engineering
            Derive new features (e.g., rooms_per_house = total_rooms / households).
            Group rare categories into ‚ÄúOther.‚Äù

        Combining specific attributes
            Interaction features (e.g., income √ó education).
            Aggregate features (e.g., population per household).

        Reusable in Pipelines
            Once written, your custom transformer can be dropped into a Scikit-Learn Pipeline or ColumnTransformer.
            Keeps preprocessing and training consistent.

# Transformation Pipelines


# Select a Performance Measure
Your next step is to select a performance measure. A typical performance measure for regression problems is the root mean square error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.


RMSE = Root Mean Square Error
–≠–Ω—ç –Ω—å –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥—ã–Ω –∑”©—Ä“Ø“Ø–≥ —Ö—ç–º–∂–∏—Ö —Ö–∞–º–≥–∏–π–Ω —Ç“Ø–≥—ç—ç–º—ç–ª –∞—Ä–≥–∞ —é–º.
    –¢–∞–∞–º–∞–≥–ª–∞–ª (predicted value) –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥–∞ (actual value) —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∑”©—Ä“Ø“Ø (error) –æ–ª–Ω–æ.
    –¢—ç—Ä –∑”©—Ä“Ø“Ø–≥ –∫–≤–∞–¥—Ä–∞—Ç –±–æ–ª–≥–æ–Ω–æ (–∏–Ω–≥—ç—Å–Ω—ç—ç—Ä —Å”©—Ä”©–≥ —É—Ç–≥–∞–≥“Ø–π –±–æ–ª–Ω–æ).
    –ë“Ø—Ö –∞–ª–¥–∞–∞–Ω—É—É–¥—ã–Ω –∫–≤–∞–¥—Ä–∞—Ç—ã–Ω –¥—É–Ω–¥–∞–∂ (mean) –∞–≤–Ω–∞.
    –≠—Ü—ç—Å—Ç –Ω—å –¥—É–Ω–¥–∞–∂ –∫–≤–∞–¥—Ä–∞—Ç—ã–≥ –¥–∞—Ö–∏–Ω “Ø—Ä–∂–≤—ç—Ä (root) –∞–≤–Ω–∞.

    –û–Ω—Ü–ª–æ–≥:
        RMSE –±–∞–≥–∞ –±–∞–π—Ö —Ç—É—Å–∞–º —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–æ–¥–∏—Ç —É—Ç–≥–∞–¥ –æ–π—Ä –±–∞–π–Ω–∞ –≥—ç—Å—ç–Ω “Ø–≥.
        RMSE –Ω—å –∞–Ω—Ö–Ω—ã —É—Ç–≥—ã–Ω –Ω—ç–≥–∂—Ç—ç–π –∏–∂–∏–ª –±–∞–π–¥–∞–≥ (–∂–∏—à—ç—ç –Ω—å, —Ö—ç—Ä–≤—ç—ç “Ø–Ω—ç —Ç”©–≥—Ä”©–≥”©”©—Ä –±–æ–ª RMSE –º”©–Ω —Ç”©–≥—Ä”©–≥”©”©—Ä —Ö—ç–º–∂–∏–≥–¥—ç–Ω—ç).