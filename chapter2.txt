# Chapter 2
Correlation is a statistical measure that shows how strongly two variables are related to each other.
If two things change together in a predictable way, they are said to be correlated.

The most common measure is the correlation coefficient (often denoted as r), which ranges from -1 to +1:
    +1 ‚Üí perfect positive correlation (when one increases, the other always increases).
    0 ‚Üí no correlation (no relationship between them).
    -1 ‚Üí perfect negative correlation (when one increases, the other always decreases)

Example:
    Positive correlation: The number of hours studied and exam score ‚Üí usually, more study hours means higher scores.
    Negative correlation: The speed of a car and the time it takes to reach a destination ‚Üí higher speed means less time.
    No correlation: Shoe size and intelligence ‚Üí unrelated.


# You create these new attributes as follows:
    df_house["rooms_per_house"] = df_house["total_rooms"] / df_house["households"]
    df_house["bedrooms_ratio"] = df_house["total_bedrooms"] / df_house["total_rooms"]
    df_house["people_per_house"] = df_house["population"] / df_house["households"]


# Clean the Data
inplace=True
    Modify the original DataFrame directly.
    If inplace=False (default), it would return a new DataFrame without changing original DataFrame.

option 1: dropna
    –î—É—Ç—É—É —É—Ç–≥–∞ –±“Ø—Ö–∏–π –º”©—Ä“Ø“Ø–¥–∏–π–≥ —É—Å—Ç–≥–∞–Ω–∞
    housing.dropna(subset=["total_bedrooms"], inplace=True) 

    Use when:
        Only a few rows are missing data.
        The missing column/row is not critical or you have enough data left.
        You want to remove NaNs specifically.

    ‚úÖ Pros: Simple, removes incomplete data.
    ‚ùå Cons: Can reduce dataset size too much if missing values are frequent.

option 2: drop
    –ë–∞–≥–∞–Ω—ã–≥ –±“Ø—Ö—ç–ª–¥ –Ω—å —É—Å—Ç–≥–∞–Ω–∞
    housing.drop("total_bedrooms", axis=1)

    Use when:
        Columns or rows are irrelevant, not missing.
        Example: ID columns, text columns that won‚Äôt be used in ML.

    ‚úÖ Pros: Removes irrelevant features, keeps dataset clean.
    ‚ùå Cons: Doesn‚Äôt handle missing values unless combined with dropna().

option 3: fillna
    Fill missing values with the median
    median = housing["total_bedrooms"].median() 
    housing["total_bedrooms"].fillna(median, inplace=True)

    Use when:
        Column is important for analysis/model.
        You cannot afford to lose rows.
        Impute missing values with a statistic or method.

option 4 Scikit-Learn class: SimpleImputer
    How it works
    When you create a SimpleImputer, you define a strategy:
        "mean" ‚Üí replaces missing values with the mean of the column (numeric data).
        "median" ‚Üí replaces missing values with the median of the column.
        "most_frequent" ‚Üí replaces with the most common value (works for both numeric & categorical).
        "constant" ‚Üí replaces with a specific value you define (e.g., 0, "missing", etc.).

    Benefits of SimpleImputer
        Prevents data loss ‚Üí Instead of dropping rows/columns with missing values, you fill them smartly.
        Keeps dataset usable ‚Üí Especially important when missing values are common.
        Different strategies ‚Üí Flexible for both numerical (mean, median) and categorical (most_frequent, constant) data.
        Pipeline integration ‚Üí Works smoothly with Scikit-Learn pipelines, so preprocessing and modeling stay consistent.

imputer = SimpleImputer(strategy="median")
housing_num = housing.select_dtypes(include=[np.number])
imputer.fit(housing_num)

Now you can use this ‚Äútrained‚Äù imputer to transform the training set by replacing missing values with the learned medians:
X = imputer.transform(housing_num)

output of imputer.transform(housing_num) is a NumPy array: X has neither column names nor index. Luckily, it‚Äôs not too hard to wrap X in a DataFrame and recover the column names and index from housing_num:
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
index=housing_num.index)

TIP
There are also more powerful imputers available in the sklearn.impute package (both fornumerical features only):
KNNImputer replaces each missing value with the mean of the k-nearest neighbors‚Äôvalues for that feature. The distance is based on all the available features.


# Handling Text and Categorical Attributes  
Most machine learning algorithms prefer to work with numbers, so let‚Äôs convert these categories from text to numbers

# Option 1 Scikit-Learn‚Äôs OrdinalEncoder class
converts categorical values (strings/labels) into integer codes.

data = pd.DataFrame({
    "city": ["Seoul", "Busan", "Daegu", "Seoul"]
})
encoder = OrdinalEncoder()
data["city_encoded"] = encoder.fit_transform(data[["city"]])\

‚úÖ Benefits:
    Very simple, compact representation (just one column).
    Works well when categories have a natural order (e.g., "low" < "medium" < "high").
    Saves memory compared to one-hot.

‚ö†Ô∏è Warning: If categories have no natural order (like city names), models may incorrectly assume that Seoul (2.0) > Busan (0.0) in a meaningful way.

# option 2 Scikit-Learn‚Äôs OneHotEncoder class
OneHotEncoder creates a binary column (0/1) for each category.
encoder = OneHotEncoder(sparse=False)
encoded = encoder.fit_transform(data[["city"]])
‚úî It creates 3 new columns: Busan, Daegu, Seoul, with 1 marking the active category.

‚úÖ Benefits:
    No false ordering ‚Üí each category is independent.
    Better for ML models (like linear regression, logistic regression) that assume numeric distance matters.
    Widely used when categories are nominal (no natural order).

‚ö†Ô∏è Downside: Increases dataset size (high-dimensional if many categories).

| Feature        | OrdinalEncoder                       | OneHotEncoder                      |
| -------------- | ------------------------------------ | ---------------------------------- |
| **Output**     | Single integer per category          | Multiple binary columns            |
| **Best for**   | Ordered categories (low/medium/high) | Unordered categories (city, color) |
| **Memory**     | Efficient (1 column)                 | More memory (n columns)            |
| **Model risk** | May mislead model if no real order   | Safe, no false order               |


# Feature Scaling and Transformation
# Feature Scaling
the numerical features in your dataset have been transformed to be on a similar scale.
Feature Scaling –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥—ã–Ω —É—Ç–≥—É—É–¥—ã–≥ –Ω—ç–≥ —Ö—ç–º–∂—ç—ç—Å —Ä“Ø“Ø –æ—Ä—É—É–ª–∞—Ö “Ø–π–ª —è–≤—Ü —é–º. –£—á–∏—Ä –Ω—å –∑–∞—Ä–∏–º –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º—É—É–¥ (gradient descent, k-NN, SVM, neural networks) ”©–≥”©–≥–¥–ª–∏–π–Ω —É—Ç–≥—ã–Ω —Ö—ç–º–∂—ç—ç—Å—ç—ç—Å —Ö–∞–º–∞–∞—Ä—á –∞–∂–∏–ª–ª–∞–¥–∞–≥.

machine learning algorithms don‚Äôt perform well when the input numerical attributes have very different scales.
    –ñ–∏—à—ç—ç:
        –ë–∞–π—à–∏–Ω–≥–∏–π–Ω —Ö—ç–º–∂—ç—ç (square feet): 0 ‚Äì 5000
        –£–Ω—Ç–ª–∞–≥—ã–Ω ”©—Ä”©”©–Ω–∏–π —Ç–æ–æ: 0 ‚Äì 5
Purpose: Prevent features with large values (e.g., income in thousands) from dominating smaller features (e.g., age).
There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.
    –ï—Ä”©–Ω—Ö–∏–π –∞—Ä–≥–∞:
        Min-Max(Normalization) Scaling (values between 0 and 1)
        Standardization –ë–∞–≥–∞–Ω—ã–≥ mean = 0, std = 1 –±–æ–ª–≥–æ–Ω–æ.
        Max Abs Scaling (Sparse ”©–≥”©–≥–¥”©–ª–¥ —Ç–æ—Ö–∏—Ä–æ–º–∂—Ç–æ–π)
    –ë–∞–≥–∞–Ω—ã–Ω —É—Ç–≥—ã–≥ [-1,1] —Ö“Ø—Ä—Ç—ç–ª –∂–∏–≥–¥–ª—ç–Ω—ç.

    Min-max scaling:
        from sklearn.preprocessing import MinMaxScaler
        min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
        housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
    Standardization scaling:
        std_scaler = StandardScaler()
        housing_num_std_scaled = std_scaler.fit_transform(housing_num)


# Feature Transformation
Feature transformation is the process of changing the representation of data features to make them more suitable for machine learning models.

It doesn‚Äôt create new information but reshapes existing features to:
    Make patterns easier to learn
    Improve model accuracy
    Speed up training

Why do we need it?
    Raw data is often messy: different scales, distributions, and types of features.
    Transforming features can:
        Handle skewed data
        Reduce the effect of outliers
        Improve convergence of algorithms (especially gradient-based)
        Make relationships more linear
        Prepare data for specific models

üîπ Common Types of Feature Transformations
    1. Scaling
        Rescale features so they have similar ranges.
        Min-Max Scaling ‚Üí transforms values into [0,1].
        Standardization (Z-score) ‚Üí mean = 0, std = 1.
    2. Normalization
        Transform each sample to have unit length (used in text/embedding data).
        Good for models using cosine similarity.
    3. Log / Power Transformation
        Fix skewed distributions (long tail values).
        Example: income, population.
            Income = [10, 100, 10000] ‚Üí log(Income) = [1, 2, 4]
    4. Bucketizing / Binning
        Convert continuous values into discrete intervals (e.g., age groups, income ranges).
    5. Encoding Categorical Data
        OrdinalEncoder ‚Üí maps categories to numbers with order.
        OneHotEncoder ‚Üí creates binary columns per category.
    6. Polynomial / Interaction Features
        Create new features by combining existing ones.
        üìå Example: x1 * x2, x1^2 ‚Üí helps linear models capture non-linear patterns.
    7. Dimensionality Reduction
        Transforms high-dimensional features into fewer dimensions (while keeping most info).
            PCA (Principal Component Analysis)
            t-SNE / UMAP (for visualization)
    
üí° –î“Ø–≥–Ω—ç–ª—Ç:
    Scaling ‚Üí –ê–ª–≥–æ—Ä–∏—Ç–º—É—É–¥—ã–≥ –∑”©–≤ –∞–∂–∏–ª–ª—É—É–ª–∞—Ö–∞–¥ —á—É—Ö–∞–ª
    Transformation ‚Üí model –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–≥–¥–¥–∞–≥

the values are shifted
1Ô∏è‚É£ Mean shift (–¥—É–Ω–¥–∞–∂ —à–∏–ª–∂“Ø“Ø–ª—ç–ª—Ç)
–ú—ç–¥—ç—ç–ª–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥—ã–Ω –¥—É–Ω–¥–∞–∂ (mean) –Ω—å 0-–¥ –æ–π—Ä –±–∞–π—Ö–∞–∞—Ä shift —Ö–∏–π–≥–¥–¥—ç–≥.
    –ñ–∏—à—ç—ç:
        Original feature: [2, 4, 6, 8]
    Mean = 5
        Shifted (centered) = [2-5, 4-5, 6-5, 8-5] = [-3, -1, 1, 3]  
“Æ“Ø–Ω–∏–π–≥ Mean normalization –≥—ç–∂ –Ω—ç—Ä–ª—ç–¥—ç–≥.
    –£—á–∏—Ä –Ω—å:
        ML –∞–ª–≥–æ—Ä–∏—Ç–º—É—É–¥ (–∂–∏—à—ç—ç –Ω—å: gradient descent, PCA) –Ω—å centered data-–¥ —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª–∞–¥–∞–≥.


# Custom Transformers
    In Scikit-Learn, a Transformer is an object that:
        Learns something from data using .fit() (e.g., mean, scaling parameters).
        Applies a transformation to the data using .transform().
        Can do both in one step with .fit_transform().

    Examples of built-in transformers:
        StandardScaler ‚Üí standardizes features.
        SimpleImputer ‚Üí fills missing values.
        OneHotEncoder ‚Üí encodes categorical features.

They are the building blocks of preprocessing pipelines.

# Why Write Custom Transformers?
Real datasets are messy. Pre-built tools don‚Äôt always fit your domain-specific needs.
That‚Äôs why you might need custom transformers.

    ‚úÖ Reasons:
        Custom cleanup operations
            Remove unwanted symbols (e.g., $ in price).
            Fix invalid values (negative ages).

        Custom feature engineering
            Derive new features (e.g., rooms_per_house = total_rooms / households).
            Group rare categories into ‚ÄúOther.‚Äù

        Combining specific attributes
            Interaction features (e.g., income √ó education).
            Aggregate features (e.g., population per household).

        Reusable in Pipelines
            Once written, your custom transformer can be dropped into a Scikit-Learn Pipeline or ColumnTransformer.
            Keeps preprocessing and training consistent.


# PIPELINE
A sequence of data processing components is called a data pipeline.Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.
”®–≥”©–≥–¥”©–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ data pipeline –≥—ç–∂ –Ω—ç—Ä–ª—ç–¥—ç–≥.–ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Å–∏—Å—Ç–µ–º–¥ –∏–π–º pipeline –º–∞—à —Ç“Ø–≥—ç—ç–º—ç–ª –±–∞–π–¥–∞–≥. –£—á–∏—Ä –Ω—å –∏—Ö —Ö—ç–º–∂—ç—ç–Ω–∏–π ”©–≥”©–≥–¥–ª–∏–π–≥ –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö, –æ–ª–æ–Ω —Ç”©—Ä–ª–∏–π–Ω —Ö—É–≤–∏—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–∞–π–¥–∞–≥.

Pipeline –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–≥ –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –∞–ª—Ö–∞–º—É—É–¥–∞–∞—Ä –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö —è–≤—Ü.

    –ñ–∏—à—ç—ç –Ω—å:
        –¢“Ø“Ø—Ö–∏–π ”©–≥”©–≥–¥”©–ª —Ü—É–≥–ª—É—É–ª–∞—Ö
        ”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç—Ö
        –®–∏–Ω–∂ —á–∞–Ω–∞—Ä (features) –≥–∞—Ä–≥–∞–∂ –∞–≤–∞—Ö
    –ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∞—Ö:
         –≠–Ω—ç –±“Ø—Ö–∏–π–≥ –Ω—ç–≥ —É—Ä—Å–≥–∞–ª (pipeline) –±–æ–ª–≥–æ—Ö –Ω—å –∞–≤—Ç–æ–º–∞—Ç–∂—É—É–ª–∂, –∞–ª–¥–∞–∞ –±–∞–≥–∞—Å–≥–∞–∂, –¥–∞—Ö–∏–Ω –∞—à–∏–≥–ª–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä –±–æ–ª–≥–æ–¥–æ–≥.

Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output.


Synchronous (—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –Ω—ç–≥ –±“Ø—Ä–¥—ç–ª –¥—É—É—Å–∞—Ö–∞–¥ –¥–∞—Ä–∞–∞–≥–∏–π–Ω—Ö —ç—Ö—ç–ª–¥—ç–≥.
Asynchronous (–∞—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ –∑–∞–∞–≤–∞–ª —Ö“Ø–ª—ç—ç–ª–≥“Ø–π, –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —ç—Å–≤—ç–ª –±–∏–µ –¥–∞–∞–Ω –∞–∂–∏–ª–ª–∞–∂ –±–æ–ª–Ω–æ.

    –ñ–∏—à—ç—ç –Ω—å, data pipeline –¥–æ—Ç–æ—Ä:
        ”®–≥”©–≥–¥”©–ª —É–Ω—à–∏–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        ”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        –ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        
–≠–¥–≥—ç—ç—Ä –Ω—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω –∞–∂–∏–ª–ª–∞–≤–∞–ª, –Ω—ç–≥ –Ω—å –¥—É—É—Å–∞—Ö–∞–∞—Ä –Ω”©–≥”©”© –Ω—å —ç—Ö–ª—ç—Ö–∏–π–≥ —Ö“Ø–ª—ç—ç—Ö–≥“Ø–π, ”©–≥”©–≥–¥”©–ª –∏—Ä—ç—Ö—Ç—ç–π –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —à—É—É–¥ –∞–∂–∏–ª–ª–∞–∂ —á–∞–¥–Ω–∞. –≠–Ω—ç –Ω—å —Ö—É—Ä–¥, “Ø—Ä –∞—à–≥–∏–π–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª–¥—ç–≥.

# Transformation Pipelines
In machine learning and data preprocessing, a transformation pipeline is a sequence (pipeline) of data transformation steps applied to raw data before feeding it into a machine learning model.
    Think of it as an assembly line:
        Raw data enters.
        Each step applies a transformation (cleaning, encoding, scaling, etc.).
        The final output is a processed dataset ready for the model.

    In Scikit-Learn, this is usually implemented using the Pipeline class.

#Why do we use Transformation Pipelines?
    ‚úÖ Consistency ‚Äì ensures that the same transformations are applied in training and testing (no mismatch).
    ‚úÖ Automation ‚Äì once defined, preprocessing is automatic, reducing manual coding.
    ‚úÖ Reproducibility ‚Äì transformations are stored in one object, so experiments can be repeated easily.
    ‚úÖ Avoids data leakage ‚Äì transformations (like scaling or imputing) are fitted only on training data, then applied to test data safely.
    ‚úÖ Integration ‚Äì combines preprocessing and modeling into one workflow (you can even grid search over the pipeline).

    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression

    # Define pipeline
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),   # Handle missing values
        ('scaler', StandardScaler()),                  # Scale features
        ('model', LogisticRegression())                # Train model
    ])

    # Fit on training data
    pipeline.fit(X_train, y_train)

    # Predict on test data
    y_pred = pipeline.predict(X_test)

    Here:
        Missing values are filled in.
        Data is scaled.
        Logistic Regression is trained ‚Äî all in one pipeline.


# Train and Evaluate on the Training Set

# Select a Performance Measure
Your next step is to select a performance measure. A typical performance measure for regression problems is the root mean square error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.


RMSE = Root Mean Square Error
–≠–Ω—ç –Ω—å –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥—ã–Ω –∑”©—Ä“Ø“Ø–≥ —Ö—ç–º–∂–∏—Ö —Ö–∞–º–≥–∏–π–Ω —Ç“Ø–≥—ç—ç–º—ç–ª –∞—Ä–≥–∞ —é–º.
    –¢–∞–∞–º–∞–≥–ª–∞–ª (predicted value) –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥–∞ (actual value) —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∑”©—Ä“Ø“Ø (error) –æ–ª–Ω–æ.
    –¢—ç—Ä –∑”©—Ä“Ø“Ø–≥ –∫–≤–∞–¥—Ä–∞—Ç –±–æ–ª–≥–æ–Ω–æ (–∏–Ω–≥—ç—Å–Ω—ç—ç—Ä —Å”©—Ä”©–≥ —É—Ç–≥–∞–≥“Ø–π –±–æ–ª–Ω–æ).
    –ë“Ø—Ö –∞–ª–¥–∞–∞–Ω—É—É–¥—ã–Ω –∫–≤–∞–¥—Ä–∞—Ç—ã–Ω –¥—É–Ω–¥–∞–∂ (mean) –∞–≤–Ω–∞.
    –≠—Ü—ç—Å—Ç –Ω—å –¥—É–Ω–¥–∞–∂ –∫–≤–∞–¥—Ä–∞—Ç—ã–≥ –¥–∞—Ö–∏–Ω “Ø—Ä–∂–≤—ç—Ä (root) –∞–≤–Ω–∞.

    –û–Ω—Ü–ª–æ–≥:
        RMSE –±–∞–≥–∞ –±–∞–π—Ö —Ç—É—Å–∞–º —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–æ–¥–∏—Ç —É—Ç–≥–∞–¥ –æ–π—Ä –±–∞–π–Ω–∞ –≥—ç—Å—ç–Ω “Ø–≥.
        RMSE –Ω—å –∞–Ω—Ö–Ω—ã —É—Ç–≥—ã–Ω –Ω—ç–≥–∂—Ç—ç–π –∏–∂–∏–ª –±–∞–π–¥–∞–≥ (–∂–∏—à—ç—ç –Ω—å, —Ö—ç—Ä–≤—ç—ç “Ø–Ω—ç —Ç”©–≥—Ä”©–≥”©”©—Ä –±–æ–ª RMSE –º”©–Ω —Ç”©–≥—Ä”©–≥”©”©—Ä —Ö—ç–º–∂–∏–≥–¥—ç–Ω—ç).

# Fine-Tune Your Model
fine-tuning your model means taking an already trained model (often a large, general-purpose one) and making small adjustments so it performs better on a specific dataset or task.It‚Äôs like starting with a ‚Äúpre-trained brain‚Äù and then customizing it for your problem instead of training everything from scratch.

üîë Key Ideas in Fine-Tuning
    Pre-trained model
        You start with a model already trained on a large dataset (e.g., ImageNet for vision, Wikipedia text for NLP).
        This model has already learned general features or patterns.

    Task-specific adaptation
        You adjust (fine-tune) the model using your smaller, domain-specific dataset.
        Example: A model trained on general images ‚Üí fine-tuned to detect medical X-rays.

    Parameter updating
        Instead of changing all weights from scratch, you update only some layers or all weights slightly.
        Sometimes, earlier layers (basic features like edges, shapes) are frozen, while later layers are fine-tuned.

    Learning rate
        Fine-tuning uses a smaller learning rate to avoid "destroying" the useful knowledge already in the pre-trained model.

‚öñÔ∏è Why Fine-Tune?
    Efficiency ‚Üí Faster training since the model already knows general patterns.
    Better accuracy ‚Üí Pre-trained knowledge gives a strong starting point.
    Useful with small data ‚Üí You don‚Äôt need millions of samples.

# Grid Search
Grid Search is a systematic way to find the best hyperparameters for a model.

üîë Key Idea
        Every ML algorithm has hyperparameters (settings you choose before training, like learning rate, number of neighbors in KNN, depth of a decision tree, etc.).
        Instead of guessing, Grid Search tests all possible combinations of these hyperparameters in a predefined ‚Äúgrid.‚Äù
        It then selects the combination that gives the best performance (based on accuracy, F1-score, RMSE, etc.).

‚öôÔ∏è How It Works
    Define the hyperparameter space (a grid of possible values).
    Example for a Decision Tree:
        max_depth = [3, 5, 10]
        min_samples_split = [2, 5, 10]

    Train and evaluate the model for every possible combination.
        If 3 depths √ó 3 splits = 9 total models.

    Use cross-validation to reduce overfitting risk and get reliable performance estimates.
    Pick the best model with the highest score.

‚úÖ Advantages
    Finds the best hyperparameters.
    Easy to understand and implement.

‚ùå Disadvantages
    Computationally expensive (tries all combinations).
    Doesn‚Äôt scale well if there are many hyperparameters or large range



# Randomized Search
In machine learning, Randomized Search (often used as RandomizedSearchCV in scikit-learn) is a technique for hyperparameter tuning ‚Äî similar to Grid Search, but more efficient.

üîë Key Idea
    Grid Search: tries all possible combinations of hyperparameters in a predefined grid.
    Randomized Search: instead of trying everything, it samples random combinations from the hyperparameter distributions you specify.

So instead of exhaustively testing 1000 combinations, it might only test 50 randomly chosen ones ‚Äî faster but still effective.

‚öôÔ∏è How It Works
    You define a distribution or list of possible hyperparameter values.
        Example: learning rate ‚àº uniform(0.0001, 0.1), max_depth ‚àà [3, 5, 10, 20].

    RandomizedSearchCV randomly selects a fixed number of combinations (n_iter).

    Each sampled set of hyperparameters is evaluated using cross-validation.

    It keeps track of the best performing set.

‚úÖ Advantages
    Much faster than Grid Search for large search spaces.
    Can search over continuous ranges (not just fixed grids).
    Good balance between performance and efficiency.

‚ùå Disadvantages
    Doesn‚Äôt guarantee finding the absolute best combination (since it samples randomly).
    Requires choosing n_iter (more iterations = better, but slower).

Grid Search = exhaustive, slower, good for small spaces.
Randomized Search = random sampling, faster, good for large spaces.

# Ensemble Methods