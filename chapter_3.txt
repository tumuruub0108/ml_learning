Chapter 3. Classification
In this chapter we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau.

"DESCR"
A description of the dataset

"data"
The input data, usually as a 2D NumPy array

"target"
The labels, usually as a 1D NumPy array

# what is class in machine learning
In machine learning, a class is simply a category or label that a data point can belong to in a classification problem.
Think of it as the ‚Äútype‚Äù or ‚Äúgroup‚Äù you want your model to predict.

üîπ Key Points About Classes
	Discrete categories
		Classes are distinct labels, not continuous numbers.
		Example: {‚Äúspam‚Äù, ‚Äúnot spam‚Äù}, {‚Äúcat‚Äù, ‚Äúdog‚Äù, ‚Äúrabbit‚Äù}, {‚Äúdisease‚Äù, ‚Äúhealthy‚Äù}
	Depends on the problem
		The number of classes depends on the specific classification task:
			Binary classification ‚Üí 2 classes (e.g., Yes/No)
			Multi-class classification ‚Üí 3 or more classes (e.g., Cat/Dog/Rabbit)
			Multi-label classification ‚Üí each instance can belong to multiple classes at the same time
	Used for training
		During training, the model learns to map input features to the correct class.
		The classes are often represented as numbers internally, e.g., 0, 1, 2‚Ä¶

| Task               | Input            | Classes                     |
| ------------------ | ---------------- | --------------------------- |
| Email filtering    | Email content    | Spam, Not Spam              |
| Fruit recognition  | Image of a fruit | Apple, Banana, Orange       |
| Sentiment analysis | Text review      | Positive, Neutral, Negative |



We use classification in machine learning because many real-world problems involve making decisions or assigning things into categories (classes) rather than predicting continuous values.

üîë Reasons why we do classification:
	Nature of the Problem
		Many tasks are inherently categorical:
			Spam ‚ùå vs. Not Spam ‚úÖ
			Fraudulent üí≥ vs. Legitimate transaction
			Disease present ü©∫ vs. not present
			Sentiment: Positive üôÇ, Negative üôÅ, Neutral üòê
	Decision-Making
		In real applications, what we care about is the final decision, not just probabilities. Example: An email system has to decide show in inbox or move to spam
	Interpretability
		Classification provides clear labels that are easy for humans to understand and act on.
	Efficiency in Action
		ystems can take automated actions once a class is determined. Example: If a transaction is classified as fraud ‚Üí block it.
	Generalization of Knowledge
		Classification allows machines to generalize from past labeled examples to unseen ones.

üß† Difference vs. Regression
	Regression ‚Üí predicts how much (continuous value).
	Classification ‚Üí predicts which category (discrete label).

	Example:
		Regression: "What is the temperature tomorrow?" ‚Üí 27.3¬∞C
		Classification: "Will it rain tomorrow?" ‚Üí Yes/No
		
# Binary Classifier
A binary classifier is a type of machine learning model that predicts one of two possible classes (also called labels) for a given input.

It‚Äôs the simplest form of classification because there are only two outcomes: usually labeled as 0 or 1, True or False, Yes or No, Positive or Negative, etc.

üîπ How it works
		The model takes input features X (like age, income, symptoms, pixel values, etc.).
		it outputs a probability P(y=1‚à£X) that the input belongs to class 1.
		A threshold (usually 0.5) is used to assign a class:
			If P(y=1‚à£X) ‚â• 0.5, predict class 1
			If P(y=1‚à£X) < 0.5, predict class 0
	examples
	| Problem              | Input                  | Output                     |
	| -------------------- | ---------------------- | -------------------------- |
	| Email spam detection | Email text             | Spam (1) / Not spam (0)    |
	| Disease diagnosis    | Symptoms, test results | Sick (1) / Healthy (0)     |
	| Fraud detection      | Transaction details    | Fraud (1) / Legitimate (0) |
	| Credit approval      | Applicant info         | Approve (1) / Reject (0)   |

	Key Points
		Works only for two classes.
		Outputs can be hard labels (0/1) or probabilities (between 0 and 1).
		Thresholds can be adjusted if you want to be more sensitive (catch all positives) or specific (avoid false alarms).


# Training a Binary Classifier
Let‚Äôs simplify the problem for now and only try to identify one digit‚Äîfor example, the number 5. This ‚Äú5-detector‚Äù will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and non-5

Now let‚Äôs pick a classifier and train it. A good place to start is with a stochastic gradient descent (SGD, or stochastic GD) classifier, using ScikitLearn‚Äôs SGDClassifier class. This classifier is capable of handling very large datasets efficiently



Performance Measures
Evaluating a classifier is often significantly trickier than evaluating a regressor

1. Measuring Accuracy Using Cross-Validation
A good way to evaluate a model is to use cross-validation, just as you did in Chapter 2. Let‚Äôs use the cross_val_score() function to evaluate our SGDClassifier model, using k-fold cross-validation with three folds.


# Confusion Matrices
A Confusion Matrix is a table used to evaluate the performance of a classification model in machine learning.
It compares the predicted labels with the actual labels to show how many predictions were correct and how many were wrong
It usually has four main terms:

True Positive (TP): Model predicted positive, and it was actually positive.
True Negative (TN): Model predicted negative, and it was actually negative.
False Positive (FP): Model predicted positive, but it was actually negative (Type I error).
False Negative (FN): Model predicted negative, but it was actually positive (Type II error).

From this matrix, we can calculate important metrics such as accuracy, precision, recall, and F1-score.

Confusion Matrix (–±—É–¥–ª–∏–∞–Ω—ã –º–∞—Ç—Ä–∏—Ü) –Ω—å –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω classification model-–∏–π–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ “Ø–Ω—ç–ª—ç—Ö—ç–¥ –∞—à–∏–≥–ª–∞–¥–∞–≥ —Ö“Ø—Å–Ω—ç–≥—Ç —é–º.
–≠–Ω—ç –Ω—å –∑–∞–≥–≤–∞—Ä—ã–Ω —Ç–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω “Ø—Ä –¥“Ø–Ω–≥ –±–æ–¥–∏—Ç “Ø—Ä –¥“Ø–Ω—Ç—ç–π —Ö–∞—Ä—å—Ü—É—É–ª–∂, –∑”©–≤ –±–æ–ª–æ–Ω –±—É—Ä—É—É –∞–Ω–≥–∏–ª—Å–∞–Ω —Ç–æ–æ–Ω—É—É–¥—ã–≥ —Ö–∞—Ä—É—É–ª–¥–∞–≥.

–ò—Ö—ç–≤—á–ª—ç–Ω 4 “Ø–Ω–¥—Å—ç–Ω —Ö—ç—Å—ç–≥—Ç—ç–π:
True Positive (TP): –ó–∞–≥–≤–∞—Ä —ç–µ—Ä—ç–≥ –≥—ç–∂ —Ç–∞–∞–º–∞–≥–ª–∞–∂, “Ø–Ω—ç—Ö—ç—ç—Ä —ç–µ—Ä—ç–≥ –±–∞–π—Å–∞–Ω.
True Negative (TN): –ó–∞–≥–≤–∞—Ä —Å”©—Ä”©–≥ –≥—ç–∂ —Ç–∞–∞–º–∞–≥–ª–∞–∂, “Ø–Ω—ç—Ö—ç—ç—Ä —Å”©—Ä”©–≥ –±–∞–π—Å–∞–Ω.
False Positive (FP): –ó–∞–≥–≤–∞—Ä —ç–µ—Ä—ç–≥ –≥—ç–∂ —Ç–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —á “Ø–Ω—ç–Ω–¥—ç—ç —Å”©—Ä”©–≥ –±–∞–π—Å–∞–Ω (I —Ç”©—Ä–ª–∏–π–Ω –∞–ª–¥–∞–∞).
False Negative (FN): –ó–∞–≥–≤–∞—Ä —Å”©—Ä”©–≥ –≥—ç–∂ —Ç–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —á “Ø–Ω—ç–Ω–¥—ç—ç —ç–µ—Ä—ç–≥ –±–∞–π—Å–∞–Ω (II —Ç”©—Ä–ª–∏–π–Ω –∞–ª–¥–∞–∞).

–≠–Ω—ç –º–∞—Ç—Ä–∏—Ü–∞–∞—Å “Ø–Ω—ç–Ω –∑”©–≤ (accuracy), –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª (precision), —Å—ç—Ä–≥—ç—ç–Ω —Ç–∞–Ω–∏–ª—Ç (recall), F1 –æ–Ω–æ–æ –∑—ç—Ä—ç–≥ —Ö—ç–º–∂“Ø“Ø—Ä“Ø“Ø–¥–∏–π–≥ —Ç–æ–æ—Ü–æ–æ–ª–¥–æ–≥.



TP (True Positive): Model correctly predicts positive.
TN (True Negative): Model correctly predicts negative.
FP (False Positive): Model incorrectly predicts positive.
FN (False Negative): Model incorrectly predicts Negative


# Accuracy
The proportion of correctly classified instances (both positive and negative) among all instance
–ë“Ø—Ö ”©–≥”©–≥–¥–ª”©”©—Å –∑”©–≤ –∞–Ω–≥–∏–ª—Å–∞–Ω —Ç–æ—Ö–∏–æ–ª–¥–ª—ã–Ω —Ö—É–≤—å.

Formula
Accuracy=TP+TN / TP+TN+FP+FN+FN


‚úÖ Good when classes are balanced.
‚ùå Misleading when classes are imbalanced

# Precision
Out of all predicted positives, how many were actually positive?
model —ç–µ—Ä—ç–≥ –≥—ç–∂ —Ç–∞–∞—Å–∞–Ω –±“Ø—Ö —Ç–æ—Ö–∏–æ–ª–¥–ª–æ–æ—Å —è–≥ “Ø–Ω—ç–Ω–¥—ç—ç —Ö—ç–¥ –Ω—å “Ø–Ω—ç—Ö—ç—ç—Ä —ç–µ—Ä—ç–≥ –±–∞–π—Å–∞–Ω

Formula
Precision=TP / TP + FP
	‚Äã
‚úÖ High precision = few false positives.
Example: In spam detection, precision measures how many emails flagged as spam are really spam.

# Recall
Out of all actual positives, how many were correctly predicted?
–ë–æ–¥–∏—Ç —ç–µ—Ä—ç–≥ –±“Ø—Ö —Ç–æ—Ö–∏–æ–ª–¥–ª–æ–æ—Å —Ö—ç–¥–∏–π–≥ –Ω—å –∑”©–≤ –∏–ª—Ä“Ø“Ø–ª—Å—ç–Ω –±—ç?

Formula
Recall= TP / TP+FN

‚úÖ High recall = few false negatives.
Example: In medical tests, recall measures how many patients with a disease are correctly identified.
	‚Äã
# F1-Score
The harmonic mean of precision and recall. It balances the two, especially when data is imbalanced.
Precision –±–∞ Recall-–∏–π–Ω —Ç—ç–Ω—Ü–≤—ç—Ä–∏–π–≥ –∏–ª—ç—Ä—Ö–∏–π–ª—ç—Ö “Ø–∑“Ø“Ø–ª—ç–ª—Ç.

Formula
F1 = 2* Precision * Recall / Precision + Recall
High F1 means a good balance between precision and recall.

Suppose you have a model detecting cancer:

100 patients
20 actually have cancer
Model predicts 25 positive cases, of which 15 are correct.

TP = 15 (correctly predicted cancer)
FP = 10 (healthy predicted as cancer)
FN = 5 (cancer missed)
TN = 70 (healthy correctly predicted)

Now:
Accuracy: (15+70)/100 = 85%
Precision: 15/(15+10) = 60%
Recall: 15/(15+5) = 75%
F1: 2 √ó (0.6√ó0.75)/(0.6+0.75) = 66.7%

Unfortunately, you can‚Äôt have it both ways: increasing precision reduces
recall, and vice versa. This is called the precision/recall trade-off.

# The Precision/Recall Trade-off
# Threshold
threshold is a cutoff value used to make decisions based on model outputs.
Most machine learning models, especially classification models, output probabilities or scores rather than hard class labels. The threshold determines how those probabilities are converted into final predictions.

cutoff = ‚Äú–±–æ—Å–≥–æ —É—Ç–≥–∞"

Example: Binary Classification
	Suppose a model predicts the probability that an email is spam:
		Email A ‚Üí 0.9 (90% spam)
		Email B ‚Üí 0.4 (40% spam)
	If the threshold is set to 0.5:
		Email A ‚Üí classified as spam (0.9 ‚â• 0.5)
		Email B ‚Üí classified as not spam (0.4 < 0.5)
	If we change the threshold to 0.3:
		Email A ‚Üí still spam
		Email B ‚Üí now also classified as spam


Machine Learning-–¥ (threshold = cutoff):
	–•—ç—Ä—ç–≤ –º–∞–≥–∞–¥–ª–∞–ª 0.5-–∞–∞—Å –¥—ç—ç—à –±–æ–ª ‚Üí —ç–µ—Ä—ç–≥ (Positive).
		0.5-–∞–∞—Å –±–∞–≥–∞ –±–æ–ª ‚Üí —Å”©—Ä”©–≥ (Negative).
		–≠–Ω—ç 0.5 –≥—ç–¥—ç–≥ –Ω—å cutoff (threshold).
	–ï—Ä–¥–∏–π–Ω –∞–º—å–¥—Ä–∞–ª –¥—ç—ç—Ä:
		–ò—Ö —Å—É—Ä–≥—É—É–ª–∏–π–Ω —à–∞–ª–≥–∞–ª—Ç–∞–Ω–¥ 60 –æ–Ω–æ–æ –∞–≤–∞—Ö —ë—Å—Ç–æ–π –±–æ–ª:
			60-–∞–∞—Å –¥—ç—ç—à ‚Üí —Ç—ç–Ω—Ü—Å—ç–Ω
			60-–∞–∞—Å –¥–æ–æ—à ‚Üí —Ç—ç–Ω—Ü—ç—ç–≥“Ø–π
		–≠–Ω—ç 60 –æ–Ω–æ–æ = cutoff —Ü—ç–≥.

Why Threshold Matters
	Higher threshold ‚Üí model is stricter (fewer positives, more false negatives).
	Lower threshold ‚Üí model is more lenient (more positives, more false positives)

Threshold = the decision boundary that converts model scores/probabilities into class labels.

you can call its decision_function() method, which returns a score for each instance, and then use any threshold
you want to make predictions based on those scores:

>>> y_scores = sgd_clf.decision_function([some_digit])
>>> y_scores
array([2164.22030239])

>>> threshold = 0
>>> y_some_digit_pred = (y_scores > threshold)
array([ True])

The SGDClassifier uses a threshold equal to 0, so the preceding code returns
the same result as the predict() method (i.e., True). Let‚Äôs raise the threshold:

>>> threshold = 3000
>>> y_some_digit_pred = (y_scores > threshold)
>>> y_some_digit_pred
array([False])

This confirms that raising the threshold decreases recall.

How do you decide which threshold to use? First, use the cross_val_predict() function to get the scores of all instances in the training set, but this time specify that you want to return decision scores instead of predictions:

y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method="decision_function")

With these scores, use the precision_recall_curve() function to compute precision and recall for all possible thresholds (the function adds a last precision of 0 and a last recall of 1, corresponding to an infinite threshold):

from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)


# The ROC Curve
The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate (FPR). The FPR (also called the fall-out) is the ratio of negative instances that are
incorrectly classified as positive. It is equal to 1 ‚Äì the true negative rate (TNR), which is the ratio of negative instances that are correctly classified as negative. The TNR is also called specificity. Hence, the ROC curve plots sensitivity (recall) versus 1 ‚Äì specificity.




TIP
Since the ROC curve is so similar to the precision/recall (PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve. For example, looking at the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear that the classifier has room for improvement: the curve could really be closer to the top-right corner


You now know how to train binary classifiers, choose the appropriate metric for your task, evaluate your classifiers using cross-validation, select the precision/recall trade-off that fits your needs, and use several metrics and curves to compare various models.

# Multiclass Classification

Multiclass classification is a type of classification in machine learning where a model predicts one of three or more possible classes for each input.

Unlike binary classification, which has only two classes, multiclass classification deals with multiple categories.

üîπ Key Points
	Single label per instance
		Each data point belongs to exactly one class out of all possible classes.

	Examples of multiclass problems
		| Problem                       | Input            | Classes                      |
		| ----------------------------- | ---------------- | ---------------------------- |
		| Handwritten digit recognition | Image of a digit | 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 |
		| Fruit classification          | Fruit image      | Apple, Banana, Orange, Mango |
		| Sentiment analysis            | Text review      | Positive, Neutral, Negative  |

	Output format
		Models usually output probabilities for each class, summing to 1.
		The predicted class is the one with the highest probability


Example
For a fruit image:

| Class  | Probability |
| ------ | ----------- |
| Apple  | 0.10        |
| Banana | 0.70        |
| Orange | 0.20        |
The model predicts Banana, because it has the highest probability.


# Multilabel Classification
Multilabel classification is a type of machine learning problem where each instance (data point) can belong to multiple classes at the same time, instead of being restricted to just one.

üìå Example
	Binary classification: Is this email spam or not spam? (one label only)
	Multiclass classification: What digit is this image? (0‚Äì9, only one correct digit)
	Multilabel classification: What topics does this news article cover?
		Possible labels: ["Politics", "Economy", "Sports", "Technology"]
		A single article could be tagged as ["Politics", "Economy"] at the same time.

üîë Characteristics
	Multiple labels per instance (not mutually exclusive).
	Output is a vector of labels (e.g., [1, 0, 1, 0] meaning it belongs to class 1 and 3, but not 2 or 4).
	Evaluation metrics differ from standard classification (accuracy, precision, recall, F1 are often adapted to multilabel cases).

‚öôÔ∏è How it works
There are a few strategies to handle multilabel classification:
	Problem Transformation Methods
		Binary Relevance: Train one binary classifier per label (e.g., one model predicts "Sports or not", another predicts "Politics or not").
		Classifier Chains: Predict labels sequentially, where the prediction of one label can influence the next.
		Label Powerset: Treat every unique combination of labels as one class (can be infeasible with many labels).

	Algorithm Adaptation
		Some algorithms are adapted directly to handle multilabel outputs, e.g.:
			Neural networks with a sigmoid activation for each output node.
			Decision trees/Random forests with multilabel extensions.




			
			
# Multioutput Classification
Multioutput classification (also called multi-output multiclass classification) is a type of supervised learning where:
	Each instance (data point) has multiple target variables (outputs).
	Each target is itself a classification problem.

In other words, you predict several labels at once, but each label comes from its own set of possible classes.

üìå Example
Handwritten digits (multioutput)
	Input: an image of a handwritten digit.
	Outputs:
		First classifier predicts the digit value (0‚Äì9).
		Second classifier predicts whether the digit is even/odd.
		Third classifier predicts whether the digit is prime or not.
		‚Üí Multiple classification outputs for the same input.
Weather prediction
	Input: historical weather data.
	Outputs:
		Temperature level (Low/Medium/High).
		Rain chance (Yes/No).
		Wind speed category (Weak/Strong).

Each output is a separate classification problem, but the model predicts them simultaneously.

üîë Difference from Multilabel Classification
| Feature             | Multilabel Classification                                  | Multioutput Classification                                  |
| ------------------- | ---------------------------------------------------------- | ----------------------------------------------------------- |
| Output type         | One set of labels (usually binary, like tags)              | Multiple classification tasks (can be binary or multiclass) |
| Labels per instance | Instance can belong to **multiple labels** (not exclusive) | Instance has **multiple target variables**                  |
| Example             | A news article tagged as `[Politics, Sports]`              | Predicting `[Weather=Sunny, Temp=Hot, Wind=Strong]`         |


üëâ Multilabel is actually a special case of multioutput, where each output is just binary (0/1).

‚öôÔ∏è How it works
Independent models: Train a separate classifier for each output.
Joint models: Train one model that outputs a vector of predictions. Neural networks often do this naturally (multi-output heads).