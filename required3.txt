Your first task is to use California census data to build a model of housing prices in the state.This data includes metrics such as the population, median income, and median housing price for each block group in California.

Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.

PIPELINES
A sequence of data processing components is called a data pipeline.Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.
Өгөгдөл боловсруулах дараалсан бүрдлүүдийг “data pipeline" гэж нэрлэдэг.сМашин сургалтын системд ийм pipeline маш түгээмэл байдаг. Учир нь их хэмжээний өгөгдлийг боловсруулах, олон төрлийн хувиргалт хийх шаардлагатай байдаг.

Pipeline гэдэг нь өгөгдлийг дараалсан алхамуудаар боловсруулах явц.

    Жишээ нь:
        Түүхий өгөгдөл цуглуулах
        Өгөгдөл цэвэрлэх
        Шинж чанар (features) гаргаж авах
    Загварт оруулах:
         Энэ бүхийг нэг урсгал (pipeline) болгох нь автоматжуулж, алдаа багасгаж, дахин ашиглахад хялбар болгодог.

Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output.


Synchronous (синхрон) → нэг бүрдэл дуусахад дараагийнх эхэлдэг.
Asynchronous (асинхрон) → бүрдлүүдийг заавал хүлээлгүй, зэрэгцээд эсвэл бие даан ажиллаж болно.

    Жишээ нь, data pipeline дотор:
        Өгөгдөл уншиж буй бүрдэл
        Өгөгдөл цэвэрлэж буй бүрдэл
        Загварт оруулж буй бүрдэл
        
Эдгээр нь асинхрон ажиллавал, нэг нь дуусахаар нөгөө нь эхлэхийг хүлээхгүй, өгөгдөл ирэхтэй зэрэгцээд шууд ажиллаж чадна. Энэ нь хурд, үр ашгийг нэмэгдүүлдэг.


Select a Performance Measure
Your next step is to select a performance measure. A typical performance measure for regression problems is the root mean square error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.


RMSE = Root Mean Square Error
Энэ нь моделийн таамаглал ба бодит утгын зөрүүг хэмжих хамгийн түгээмэл арга юм.
    Таамаглал (predicted value) ба бодит утга (actual value) хоорондын зөрүү (error) олно.
    Тэр зөрүүг квадрат болгоно (ингэснээр сөрөг утгагүй болно).
    Бүх алдаануудын квадратын дундаж (mean) авна.
    Эцэст нь дундаж квадратыг дахин үржвэр (root) авна.

    Онцлог:
        RMSE бага байх тусам таамаглал бодит утгад ойр байна гэсэн үг.
        RMSE нь анхны утгын нэгжтэй ижил байдаг (жишээ нь, хэрвээ үнэ төгрөгөөр бол RMSE мөн төгрөгөөр хэмжигдэнэ).

# Chapter 2
Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways ==> 
from sklearn.model_selection import train_test_split
80% train, 20% test
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

Correlation is a statistical measure that shows how strongly two variables are related to each other.
If two things change together in a predictable way, they are said to be correlated.

The most common measure is the correlation coefficient (often denoted as r), which ranges from -1 to +1:
    +1 → perfect positive correlation (when one increases, the other always increases).
    0 → no correlation (no relationship between them).
    -1 → perfect negative correlation (when one increases, the other always decreases)

Example:
    Positive correlation: The number of hours studied and exam score → usually, more study hours means higher scores.
    Negative correlation: The speed of a car and the time it takes to reach a destination → higher speed means less time.
    No correlation: Shoe size and intelligence → unrelated.


You create these new attributes as follows:
    df_house["rooms_per_house"] = df_house["total_rooms"] / df_house["households"]
    df_house["bedrooms_ratio"] = df_house["total_bedrooms"] / df_house["total_rooms"]
    df_house["people_per_house"] = df_house["population"] / df_house["households"]


Clean the Data

option 1
    Дутуу утга бүхий мөрүүдийг устгана
    housing.dropna(subset=["total_bedrooms"], inplace=True) 

option 2
    Баганыг бүхэлд нь устгана
    housing.drop("total_bedrooms", axis=1)  

option 3
    Fill missing values with the median
    median = housing["total_bedrooms"].median() 
    housing["total_bedrooms"].fillna(median, inplace=True)


You decide to go for option 3 since it is the least destructive

option 4
you will use a handy Scikit-Learn class: SimpleImputer.
The benefit is that it will store the median value of each feature:

imputer = SimpleImputer(strategy="median")
housing_num = df_house.select_dtypes(include=[np.number])
imputer.fit(housing_num)

Since the median can only be computed on numerical attributes, you thenneed to create a copy of the data with only the numerical attributes

The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable.
print(imputer.statistics_)

another option
print(df_house .median(numeric_only=True).values)

Now you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:
X = imputer.transform(housing_num)

output of imputer.transform(housing_num) is a NumPy array: X has neither column names nor index. Luckily, it’s not too hard to wrap X in a DataFrame and recover the column names and index from housing_num:
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
index=housing_num.index)

TIP
There are also more powerful imputers available in the sklearn.impute package (both fornumerical features only):
KNNImputer replaces each missing value with the mean of the k-nearest neighbors’values for that feature. The distance is based on all the available features.


# Handling Text and Categorical Attributes  
Most machine learning algorithms prefer to work with numbers, so let’s convert these categories from text to numbers
we can use Scikit-Learn’s OrdinalEncoder class:
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)

option 2
The new attributes are sometimes called dummy attributes.Scikit-Learn provides a OneHotEncoder class to convert categorical values
into one-hot vectors:

from sklearn.preprocessing import OneHotEncoder
# Handling Text and Categorical Attributes
data = pd.DataFrame({"ocean_proximity": ["NEAR BAY", "INLAND", "NEAR OCEAN", "INLAND"]})

cat_encoder = OneHotEncoder()
X = df_house[["ocean_proximity"]]
X_encoded = cat_encoder.fit_transform(X)

X_encoded_df = pd.DataFrame(
    X_encoded.toarray(),
    columns=cat_encoder.get_feature_names_out(["ocean_proximity"]),
    index=df_house.index  # now X_encoded has same number of rows as df_house
)

df_house = df_house.drop("ocean_proximity", axis=1)
df_house = pd.concat([df_house, X_encoded_df], axis=1)


By default, the output of a OneHotEncoder is a SciPy sparse matrix, instead of a NumPy array:
the advantage of OneHotEncoder is that it remembers which categories it wastrained on


# Feature Scaling and Transformation
# Feature Scaling
the numerical features in your dataset have been transformed to be on a similar scale.
Feature Scaling гэдэг нь өгөгдлийн багануудын утгуудыг нэг хэмжээс рүү оруулах үйл явц юм. Учир нь зарим машин сургалтын алгоритмууд (gradient descent, k-NN, SVM, neural networks) өгөгдлийн утгын хэмжээсээс хамаарч ажилладаг.

machine learning algorithms don’t perform well when the input numerical attributes have very different scales.
    Жишээ:
        Байшингийн хэмжээ (square feet): 0 – 5000
        Унтлагын өрөөний тоо: 0 – 5
Purpose: Prevent features with large values (e.g., income in thousands) from dominating smaller features (e.g., age).
There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.
    Ерөнхий арга:
        Min-Max(Normalization) Scaling (values between 0 and 1)
        Standardization Баганыг mean = 0, std = 1 болгоно.
        Max Abs Scaling (Sparse өгөгдөлд тохиромжтой)
    Баганын утгыг [-1,1] хүртэл жигдлэнэ.

    Min-max scaling:
        from sklearn.preprocessing import MinMaxScaler
        min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
        housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
    Standardization scaling:
        std_scaler = StandardScaler()
        housing_num_std_scaled = std_scaler.fit_transform(housing_num)

# Capped data
means extreme values (outliers) have been limited to a certain threshold.(Хэтэрхий хэтэрхий их/бага утгуудыг (outliers) тодорхой босгонд хязгаарласан.)

    Example: If most incomes are below $200k but some are $5M, you might cap all values above $200k at $200k.
    Жишээ: Хэрэв ихэнх орлого 200,000$-оос доош байхад зарим нь 5,000,000$ бол 200,000$-оос дээш утгыг бүгдийг нь 200,000 гэж авдаг.
    Purpose: Reduce the negative impact of outliers on model training.
    Зорилго: Outlier-ууд загварыг буруу сургахаас сэргийлэх.

# clustering
Clustering is the process of grouping similar data points together based on their features, without using labeled data.
When data is clustered, it means the algorithm has divided the dataset into groups (called clusters) such that:

    Points in the same cluster are more similar to each other.
    Points in different clusters are more different from each other.

Example:
    Suppose you have data about customers:
        Age
        Income
        Spending habits

    A clustering algorithm might group them into:
        Cluster 1: Young, low income, low spending

        Cluster 2: Middle-aged, medium income, medium spending

        Cluster 3: Older, high income, high spending
        
No labels are provided—the algorithm “discovers” these groups automatically.

# Stratified
In machine learning, “stratified” usually refers to stratified sampling or stratified splitting, which is a way of dividing data into train/test (or folds in cross-validation) while preserving the distribution of labels or classes.

📊 Example:
Suppose you have a dataset with 100 samples:
    80 are class A

    20 are class B

If you do a random split into 80% training and 20% testing, you might end up with:
    Train: 70 A, 10 B

    Test: 10 A, 10 B (imbalanced compared to original)

But with a stratified split, the ratio is preserved:
    Train: 64 A, 16 B (still 80:20 ratio)

    Test: 16 A, 4 B (still 80:20 ratio)

# Why it’s important:
    Prevents bias in model evaluation.
    Especially useful for imbalanced datasets (where some classes are rare).
    Helps the model generalize better and ensures fair testing.

# Feature transformations
In Machine Learning (ML), a transformation means applying some operation to your data (inputs, features, or sometimes outputs) to make it more suitable for training a model or improving performance

Feature Transformation гэдэг нь өгөгдлийн баганын утгыг шинэ хэлбэрт оруулах үйл явц юм.
    Яагаад хэрэгтэй вэ:
        Өгөгдлийн тархалтыг нормалд ойртуулж, сургалтын алгоритмын гүйцэтгэлийг сайжруулах
        Шугаман бус (non-linear) хамаарлыг засах
    Ерөнхий арга
    Log Transformation
    Баруун тийш хазайсан өгөгдлийг жигдлэх:
        X_log = np.log1p(X)
    Power / Box-Cox Transformation
    Өгөгдлийг Gaussian тархалттай ойртуулах

    from sklearn.preprocessing import PowerTransformer
    pt = PowerTransformer()
    X_transformed = pt.fit_transform(X)

Polynomial Features
Interaction буюу квадрат, куб зэрэг өндөр зэрэглэлийн баганууд нэмэх
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

    💡 Дүгнэлт:
        Scaling → Алгоритмуудыг зөв ажиллуулахад чухал
        Transformation → model гүйцэтгэлийг сайжруулахад ашиглагддаг

the values are shifted
1️⃣ Mean shift (дундаж шилжүүлэлт)
Мэдээллийн багануудын дундаж (mean) нь 0-д ойр байхаар shift хийгддэг.
    Жишээ:
        Original feature: [2, 4, 6, 8]
    Mean = 5
        Shifted (centered) = [2-5, 4-5, 6-5, 8-5] = [-3, -1, 1, 3]  
Үүнийг Mean normalization гэж нэрлэдэг.
    Учир нь:
        ML алгоритмууд (жишээ нь: gradient descent, PCA) нь centered data-д хурдан ажилладаг.


# what is heavy tail 
In Machine Learning (ML) and statistics, a heavy tail refers to a probability distribution where the probability of very large values (extreme outcomes) decreases more slowly than it would in a "normal" distribution like the Gaussian.э?

Heavy tail гэдэг нь өгөгдлийн магадлалын тархалт нь маш их утгууд (экстрем утгууд) гарах магадлал их байдаг гэсэн үг юм.

Өөрөөр хэлбэл:
Ердийн (Normal) тархалт → дундажийн орчимд утгууд их төвлөрч, маш том эсвэл маш жижиг утгууд ховор тохиолдоно.
Heavy-tailed тархалт (жишээ нь: Pareto, Cauchy, Student’s t) → маш том утгууд, онцгой (outlier) утгууд илүү олон гардаг.

Жишээ:
    Хүн амын орлого: ихэнх нь дундаж орчимд, гэхдээ маш цөөн хэд нь маш өндөр орлоготой байдаг.
    Интернэтийн сүлжээний ачаалал: ихэнх үед хэвийн, гэхдээ хааяа маш их өсөлттэй “spike” гардаг.
    Энэ бол heavy tail үзэгдэл.