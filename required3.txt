Your first task is to use California census data to build a model of housing prices in the state.This data includes metrics such as the population, median income, and median housing price for each block group in California.

Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.

PIPELINES

A sequence of data processing components is called a data pipeline.Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.
”®–≥”©–≥–¥”©–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ ‚Äúdata pipeline" –≥—ç–∂ –Ω—ç—Ä–ª—ç–¥—ç–≥. –ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Å–∏—Å—Ç–µ–º–¥ –∏–π–º pipeline –º–∞—à —Ç“Ø–≥—ç—ç–º—ç–ª –±–∞–π–¥–∞–≥. –£—á–∏—Ä –Ω—å –∏—Ö —Ö—ç–º–∂—ç—ç–Ω–∏–π ”©–≥”©–≥–¥–ª–∏–π–≥ –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö, –æ–ª–æ–Ω —Ç”©—Ä–ª–∏–π–Ω —Ö—É–≤–∏—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–∞–π–¥–∞–≥.


Pipeline –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–≥ –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –∞–ª—Ö–∞–º—É—É–¥–∞–∞—Ä –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö —è–≤—Ü.

–ñ–∏—à—ç—ç –Ω—å:

–¢“Ø“Ø—Ö–∏–π ”©–≥”©–≥–¥”©–ª —Ü—É–≥–ª—É—É–ª–∞—Ö
”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç—Ö
–®–∏–Ω–∂ —á–∞–Ω–∞—Ä (features) –≥–∞—Ä–≥–∞–∂ –∞–≤–∞—Ö
–ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∞—Ö
‚Üí –≠–Ω—ç –±“Ø—Ö–∏–π–≥ –Ω—ç–≥ —É—Ä—Å–≥–∞–ª (pipeline) –±–æ–ª–≥–æ—Ö –Ω—å –∞–≤—Ç–æ–º–∞—Ç–∂—É—É–ª–∂, –∞–ª–¥–∞–∞ –±–∞–≥–∞—Å–≥–∞–∂, –¥–∞—Ö–∏–Ω –∞—à–∏–≥–ª–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä –±–æ–ª–≥–æ–¥–æ–≥.

Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output.


Synchronous (—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –Ω—ç–≥ –±“Ø—Ä–¥—ç–ª –¥—É—É—Å–∞—Ö–∞–¥ –¥–∞—Ä–∞–∞–≥–∏–π–Ω—Ö —ç—Ö—ç–ª–¥—ç–≥.
Asynchronous (–∞—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ –∑–∞–∞–≤–∞–ª —Ö“Ø–ª—ç—ç–ª–≥“Ø–π, –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —ç—Å–≤—ç–ª –±–∏–µ –¥–∞–∞–Ω –∞–∂–∏–ª–ª–∞–∂ –±–æ–ª–Ω–æ.

–ñ–∏—à—ç—ç –Ω—å, data pipeline –¥–æ—Ç–æ—Ä:

”®–≥”©–≥–¥”©–ª —É–Ω—à–∏–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
–ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∂ –±—É–π –±“Ø—Ä–¥—ç–ª

–≠–¥–≥—ç—ç—Ä –Ω—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω –∞–∂–∏–ª–ª–∞–≤–∞–ª, –Ω—ç–≥ –Ω—å –¥—É—É—Å–∞—Ö–∞–∞—Ä –Ω”©–≥”©”© –Ω—å —ç—Ö–ª—ç—Ö–∏–π–≥ —Ö“Ø–ª—ç—ç—Ö–≥“Ø–π, ”©–≥”©–≥–¥”©–ª –∏—Ä—ç—Ö—Ç—ç–π –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —à—É—É–¥ –∞–∂–∏–ª–ª–∞–∂ —á–∞–¥–Ω–∞. –≠–Ω—ç –Ω—å —Ö—É—Ä–¥, “Ø—Ä –∞—à–≥–∏–π–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª–¥—ç–≥.



you are now ready to start designing your system.First, determine what kind of training supervision the model will need: is it a
supervised, unsupervised, semi-supervised, self-supervised, or reinforcement learning task? And is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.


Select a Performance Measure
Your next step is to select a performance measure. A typical performance measure for regression problems is the root mean square error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.

Notations
m is 


RMSE = Root Mean Square Error
–≠–Ω—ç –Ω—å –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥—ã–Ω –∑”©—Ä“Ø“Ø–≥ —Ö—ç–º–∂–∏—Ö —Ö–∞–º–≥–∏–π–Ω —Ç“Ø–≥—ç—ç–º—ç–ª –∞—Ä–≥–∞ —é–º.

–¢–∞–∞–º–∞–≥–ª–∞–ª (predicted value) –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥–∞ (actual value) —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∑”©—Ä“Ø“Ø (error) –æ–ª–Ω–æ.
–¢—ç—Ä –∑”©—Ä“Ø“Ø–≥ –∫–≤–∞–¥—Ä–∞—Ç –±–æ–ª–≥–æ–Ω–æ (–∏–Ω–≥—ç—Å–Ω—ç—ç—Ä —Å”©—Ä”©–≥ —É—Ç–≥–∞–≥“Ø–π –±–æ–ª–Ω–æ).
–ë“Ø—Ö –∞–ª–¥–∞–∞–Ω—É—É–¥—ã–Ω –∫–≤–∞–¥—Ä–∞—Ç—ã–Ω –¥—É–Ω–¥–∞–∂ (mean) –∞–≤–Ω–∞.
–≠—Ü—ç—Å—Ç –Ω—å –¥—É–Ω–¥–∞–∂ –∫–≤–∞–¥—Ä–∞—Ç—ã–≥ –¥–∞—Ö–∏–Ω “Ø—Ä–∂–≤—ç—Ä (root) –∞–≤–Ω–∞.


–û–Ω—Ü–ª–æ–≥:

RMSE –±–∞–≥–∞ –±–∞–π—Ö —Ç—É—Å–∞–º —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–æ–¥–∏—Ç —É—Ç–≥–∞–¥ –æ–π—Ä –±–∞–π–Ω–∞ –≥—ç—Å—ç–Ω “Ø–≥.
RMSE –Ω—å –∞–Ω—Ö–Ω—ã —É—Ç–≥—ã–Ω –Ω—ç–≥–∂—Ç—ç–π –∏–∂–∏–ª –±–∞–π–¥–∞–≥ (–∂–∏—à—ç—ç –Ω—å, —Ö—ç—Ä–≤—ç—ç “Ø–Ω—ç —Ç”©–≥—Ä”©–≥”©”©—Ä –±–æ–ª RMSE –º”©–Ω —Ç”©–≥—Ä”©–≥”©”©—Ä —Ö—ç–º–∂–∏–≥–¥—ç–Ω—ç).



chapter 2
The info() method is useful to get a quick description of the data ==> df.info()
You can find out what categories exist and how many districts belong to each category by using the value_counts() method ==> 
df["ocean_proximity"].value_counts()
The describe() method shows a summary of the numerical attributes ==> df.describe()
show a histogram for each numerical attribute 
df.hist(bins=50, figsize=(12, 8))
plt.show()

Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways ==> 
from sklearn.model_selection import train_test_split
80% train, 20% test
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)


The following code shows California housing prices: red is expensive, blue is cheap, and larger circles indicate areas with a larger population.
df_house.plot(kind="scatter", x="longitude", y="latitude", grid=True,
s=df_house["population"] / 100, label="population",
c="median_house_value", cmap="jet", colorbar=True,
legend=True, sharex=False, figsize=(10, 7))
plt.show()


Correlation is a statistical measure that shows how strongly two variables are related to each other.
If two things change together in a predictable way, they are said to be correlated.

The most common measure is the correlation coefficient (often denoted as r), which ranges from -1 to +1:

+1 ‚Üí perfect positive correlation (when one increases, the other always increases).
0 ‚Üí no correlation (no relationship between them).
-1 ‚Üí perfect negative correlation (when one increases, the other always decreases)

Example:

Positive correlation: The number of hours studied and exam score ‚Üí usually, more study hours means higher scores.
Negative correlation: The speed of a car and the time it takes to reach a destination ‚Üí higher speed means less time.
No correlation: Shoe size and intelligence ‚Üí unrelated.

corr_matrix = df_house.corr(numeric_only=True)
print(corr_matrix["median_house_value"].sort_values(ascending=False))

You create these new attributes as follows:

df_house["rooms_per_house"] = df_house["total_rooms"] / df_house["households"]
df_house["bedrooms_ratio"] = df_house["total_bedrooms"] / df_house["total_rooms"]
df_house["people_per_house"] = df_house["population"] / df_house["households"]


Clean the Data

option 1
–î—É—Ç—É—É —É—Ç–≥–∞ –±“Ø—Ö–∏–π –º”©—Ä“Ø“Ø–¥–∏–π–≥ —É—Å—Ç–≥–∞–Ω–∞
housing.dropna(subset=["total_bedrooms"], inplace=True) 

option 2
–ë–∞–≥–∞–Ω—ã–≥ –±“Ø—Ö—ç–ª–¥ –Ω—å —É—Å—Ç–≥–∞–Ω–∞
housing.drop("total_bedrooms", axis=1)  

option 3
Fill missing values with the median
median = housing["total_bedrooms"].median() 
housing["total_bedrooms"].fillna(median, inplace=True)


You decide to go for option 3 since it is the least destructive

option 4
you will use a handy Scikit-Learn class: SimpleImputer.
The benefit is that it will store the median value of each feature:

imputer = SimpleImputer(strategy="median")
housing_num = df_house.select_dtypes(include=[np.number])
imputer.fit(housing_num)

Since the median can only be computed on numerical attributes, you thenneed to create a copy of the data with only the numerical attributes

The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable.
print(imputer.statistics_)

another option
print(df_house .median(numeric_only=True).values)


Now you can use this ‚Äútrained‚Äù imputer to transform the training set by replacing missing values with the learned medians:
X = imputer.transform(housing_num)



output of imputer.transform(housing_num) is a NumPy array: X has neither column names nor index. Luckily, it‚Äôs not too hard to wrap X in a DataFrame and recover the column names and index from housing_num:
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
index=housing_num.index)

TIP
There are also more powerful imputers available in the sklearn.impute package (both for
numerical features only):
KNNImputer replaces each missing value with the mean of the k-nearest neighbors‚Äôvalues for that feature. The distance is based on all the available features.


Handling Text and Categorical Attributes

Most machine learning algorithms prefer to work with numbers, so let‚Äôs convert these categories from text to numbers
we can use Scikit-Learn‚Äôs OrdinalEncoder class:
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)

option 2
The new attributes are sometimes called dummy attributes.Scikit-Learn provides a OneHotEncoder class to convert categorical values
into one-hot vectors:

from sklearn.preprocessing import OneHotEncoder
# Handling Text and Categorical Attributes
data = pd.DataFrame({"ocean_proximity": ["NEAR BAY", "INLAND", "NEAR OCEAN", "INLAND"]})

cat_encoder = OneHotEncoder()
X = df_house[["ocean_proximity"]]
X_encoded = cat_encoder.fit_transform(X)

X_encoded_df = pd.DataFrame(
    X_encoded.toarray(),
    columns=cat_encoder.get_feature_names_out(["ocean_proximity"]),
    index=df_house.index  # now X_encoded has same number of rows as df_house
)

df_house = df_house.drop("ocean_proximity", axis=1)
df_house = pd.concat([df_house, X_encoded_df], axis=1)


By default, the output of a OneHotEncoder is a SciPy sparse matrix, instead of a NumPy array:
the advantage of OneHotEncoder is that it remembers which categories it wastrained on


Feature Scaling and Transformation

Feature Scaling –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥—ã–Ω —É—Ç–≥—É—É–¥—ã–≥ –Ω—ç–≥ —Ö—ç–º–∂—ç—ç—Å —Ä“Ø“Ø –æ—Ä—É—É–ª–∞—Ö “Ø–π–ª —è–≤—Ü —é–º.
–£—á–∏—Ä –Ω—å –∑–∞—Ä–∏–º –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º—É—É–¥ (gradient descent, k-NN, SVM, neural networks) ”©–≥”©–≥–¥–ª–∏–π–Ω —É—Ç–≥—ã–Ω —Ö—ç–º–∂—ç—ç—Å—ç—ç—Å —Ö–∞–º–∞–∞—Ä—á –∞–∂–∏–ª–ª–∞–¥–∞–≥.
–ñ–∏—à—ç—ç:

–ë–∞–π—à–∏–Ω–≥–∏–π–Ω —Ö—ç–º–∂—ç—ç (square feet): 0 ‚Äì 5000

–£–Ω—Ç–ª–∞–≥—ã–Ω ”©—Ä”©”©–Ω–∏–π —Ç–æ–æ: 0 ‚Äì 5

–•—ç—Ä—ç–≤ –∂–∏–≥–¥–ª—ç—Ö–≥“Ø–π –±–æ–ª —Ç–æ–º —É—Ç–≥–∞—Ç–∞–π –±–∞–≥–∞–Ω—É—É–¥ –∏–ª“Ø“Ø –∏—Ö –∞—á —Ö–æ–ª–±–æ–≥–¥–æ–ª—Ç–æ–π –≥—ç–∂ “Ø–∑—ç–≥–¥—ç—Ö –±–æ–ª–Ω–æ.

–ï—Ä”©–Ω—Ö–∏–π –∞—Ä–≥–∞
Min-Max Scaling (0‚Äì1 —Ö“Ø—Ä—Ç—ç–ª —Ö—É–≤–∏—Ä–≥–∞—Ö) (called normalization)
Standardization (Z-score) –ë–∞–≥–∞–Ω—ã–≥ mean = 0, std = 1 –±–æ–ª–≥–æ–Ω–æ.
Max Abs Scaling (Sparse ”©–≥”©–≥–¥”©–ª–¥ —Ç–æ—Ö–∏—Ä–æ–º–∂—Ç–æ–π)

–ë–∞–≥–∞–Ω—ã–Ω —É—Ç–≥—ã–≥ [-1,1] —Ö“Ø—Ä—Ç—ç–ª –∂–∏–≥–¥–ª—ç–Ω—ç.

2Ô∏è‚É£ Feature Transformation (”®–≥”©–≥–¥–ª–∏–π–≥ —Ö—É–≤–∏—Ä–≥–∞—Ö)

Feature Transformation –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–Ω –±–∞–≥–∞–Ω—ã–Ω —É—Ç–≥—ã–≥ —à–∏–Ω—ç —Ö—ç–ª–±—ç—Ä—Ç –æ—Ä—É—É–ª–∞—Ö “Ø–π–ª —è–≤—Ü —é–º.

–Ø–∞–≥–∞–∞–¥ —Ö—ç—Ä—ç–≥—Ç—ç–π –≤—ç?

”®–≥”©–≥–¥–ª–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç—ã–≥ –Ω–æ—Ä–º–∞–ª–¥ –æ–π—Ä—Ç—É—É–ª–∂, —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º—ã–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö
–®—É–≥–∞–º–∞–Ω –±—É—Å (non-linear) —Ö–∞–º–∞–∞—Ä–ª—ã–≥ –∑–∞—Å–∞—Ö

–ï—Ä”©–Ω—Ö–∏–π –∞—Ä–≥–∞

Log Transformation

–ë–∞—Ä—É—É–Ω —Ç–∏–π—à —Ö–∞–∑–∞–π—Å–∞–Ω ”©–≥”©–≥–¥–ª–∏–π–≥ –∂–∏–≥–¥–ª—ç—Ö
X_log = np.log1p(X)

Power / Box-Cox Transformation
”®–≥”©–≥–¥–ª–∏–π–≥ Gaussian —Ç–∞—Ä—Ö–∞–ª—Ç—Ç–∞–π –æ–π—Ä—Ç—É—É–ª–∞—Ö

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer()
X_transformed = pt.fit_transform(X)

Polynomial Features
Interaction –±—É—é—É –∫–≤–∞–¥—Ä–∞—Ç, –∫—É–± –∑—ç—Ä—ç–≥ ”©–Ω–¥”©—Ä –∑—ç—Ä—ç–≥–ª—ç–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥ –Ω—ç–º—ç—Ö
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)




üí° –î“Ø–≥–Ω—ç–ª—Ç:

Scaling ‚Üí –ê–ª–≥–æ—Ä–∏—Ç–º—É—É–¥—ã–≥ –∑”©–≤ –∞–∂–∏–ª–ª—É—É–ª–∞—Ö–∞–¥ —á—É—Ö–∞–ª
Transformation ‚Üí model –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–≥–¥–¥–∞–≥