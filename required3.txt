Your first task is to use California census data to build a model of housing prices in the state.This data includes metrics such as the population, median income, and median housing price for each block group in California.

Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.

PIPELINES
A sequence of data processing components is called a data pipeline.Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.
”®–≥”©–≥–¥”©–ª –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ ‚Äúdata pipeline" –≥—ç–∂ –Ω—ç—Ä–ª—ç–¥—ç–≥.—Å–ú–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω —Å–∏—Å—Ç–µ–º–¥ –∏–π–º pipeline –º–∞—à —Ç“Ø–≥—ç—ç–º—ç–ª –±–∞–π–¥–∞–≥. –£—á–∏—Ä –Ω—å –∏—Ö —Ö—ç–º–∂—ç—ç–Ω–∏–π ”©–≥”©–≥–¥–ª–∏–π–≥ –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö, –æ–ª–æ–Ω —Ç”©—Ä–ª–∏–π–Ω —Ö—É–≤–∏—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–∞–π–¥–∞–≥.

Pipeline –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–≥ –¥–∞—Ä–∞–∞–ª—Å–∞–Ω –∞–ª—Ö–∞–º—É—É–¥–∞–∞—Ä –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∞—Ö —è–≤—Ü.

    –ñ–∏—à—ç—ç –Ω—å:
        –¢“Ø“Ø—Ö–∏–π ”©–≥”©–≥–¥”©–ª —Ü—É–≥–ª—É—É–ª–∞—Ö
        ”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç—Ö
        –®–∏–Ω–∂ —á–∞–Ω–∞—Ä (features) –≥–∞—Ä–≥–∞–∂ –∞–≤–∞—Ö
    –ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∞—Ö:
         –≠–Ω—ç –±“Ø—Ö–∏–π–≥ –Ω—ç–≥ —É—Ä—Å–≥–∞–ª (pipeline) –±–æ–ª–≥–æ—Ö –Ω—å –∞–≤—Ç–æ–º–∞—Ç–∂—É—É–ª–∂, –∞–ª–¥–∞–∞ –±–∞–≥–∞—Å–≥–∞–∂, –¥–∞—Ö–∏–Ω –∞—à–∏–≥–ª–∞—Ö–∞–¥ —Ö—è–ª–±–∞—Ä –±–æ–ª–≥–æ–¥–æ–≥.

Components typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output.


Synchronous (—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –Ω—ç–≥ –±“Ø—Ä–¥—ç–ª –¥—É—É—Å–∞—Ö–∞–¥ –¥–∞—Ä–∞–∞–≥–∏–π–Ω—Ö —ç—Ö—ç–ª–¥—ç–≥.
Asynchronous (–∞—Å–∏–Ω—Ö—Ä–æ–Ω) ‚Üí –±“Ø—Ä–¥–ª“Ø“Ø–¥–∏–π–≥ –∑–∞–∞–≤–∞–ª —Ö“Ø–ª—ç—ç–ª–≥“Ø–π, –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —ç—Å–≤—ç–ª –±–∏–µ –¥–∞–∞–Ω –∞–∂–∏–ª–ª–∞–∂ –±–æ–ª–Ω–æ.

    –ñ–∏—à—ç—ç –Ω—å, data pipeline –¥–æ—Ç–æ—Ä:
        ”®–≥”©–≥–¥”©–ª —É–Ω—à–∏–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        ”®–≥”©–≥–¥”©–ª —Ü—ç–≤—ç—Ä–ª—ç–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        –ó–∞–≥–≤–∞—Ä—Ç –æ—Ä—É—É–ª–∂ –±—É–π –±“Ø—Ä–¥—ç–ª
        
–≠–¥–≥—ç—ç—Ä –Ω—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω –∞–∂–∏–ª–ª–∞–≤–∞–ª, –Ω—ç–≥ –Ω—å –¥—É—É—Å–∞—Ö–∞–∞—Ä –Ω”©–≥”©”© –Ω—å —ç—Ö–ª—ç—Ö–∏–π–≥ —Ö“Ø–ª—ç—ç—Ö–≥“Ø–π, ”©–≥”©–≥–¥”©–ª –∏—Ä—ç—Ö—Ç—ç–π –∑—ç—Ä—ç–≥—Ü—ç—ç–¥ —à—É—É–¥ –∞–∂–∏–ª–ª–∞–∂ —á–∞–¥–Ω–∞. –≠–Ω—ç –Ω—å —Ö—É—Ä–¥, “Ø—Ä –∞—à–≥–∏–π–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª–¥—ç–≥.


Select a Performance Measure
Your next step is to select a performance measure. A typical performance measure for regression problems is the root mean square error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.


RMSE = Root Mean Square Error
–≠–Ω—ç –Ω—å –º–æ–¥–µ–ª–∏–π–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥—ã–Ω –∑”©—Ä“Ø“Ø–≥ —Ö—ç–º–∂–∏—Ö —Ö–∞–º–≥–∏–π–Ω —Ç“Ø–≥—ç—ç–º—ç–ª –∞—Ä–≥–∞ —é–º.
    –¢–∞–∞–º–∞–≥–ª–∞–ª (predicted value) –±–∞ –±–æ–¥–∏—Ç —É—Ç–≥–∞ (actual value) —Ö–æ–æ—Ä–æ–Ω–¥—ã–Ω –∑”©—Ä“Ø“Ø (error) –æ–ª–Ω–æ.
    –¢—ç—Ä –∑”©—Ä“Ø“Ø–≥ –∫–≤–∞–¥—Ä–∞—Ç –±–æ–ª–≥–æ–Ω–æ (–∏–Ω–≥—ç—Å–Ω—ç—ç—Ä —Å”©—Ä”©–≥ —É—Ç–≥–∞–≥“Ø–π –±–æ–ª–Ω–æ).
    –ë“Ø—Ö –∞–ª–¥–∞–∞–Ω—É—É–¥—ã–Ω –∫–≤–∞–¥—Ä–∞—Ç—ã–Ω –¥—É–Ω–¥–∞–∂ (mean) –∞–≤–Ω–∞.
    –≠—Ü—ç—Å—Ç –Ω—å –¥—É–Ω–¥–∞–∂ –∫–≤–∞–¥—Ä–∞—Ç—ã–≥ –¥–∞—Ö–∏–Ω “Ø—Ä–∂–≤—ç—Ä (root) –∞–≤–Ω–∞.

    –û–Ω—Ü–ª–æ–≥:
        RMSE –±–∞–≥–∞ –±–∞–π—Ö —Ç—É—Å–∞–º —Ç–∞–∞–º–∞–≥–ª–∞–ª –±–æ–¥–∏—Ç —É—Ç–≥–∞–¥ –æ–π—Ä –±–∞–π–Ω–∞ –≥—ç—Å—ç–Ω “Ø–≥.
        RMSE –Ω—å –∞–Ω—Ö–Ω—ã —É—Ç–≥—ã–Ω –Ω—ç–≥–∂—Ç—ç–π –∏–∂–∏–ª –±–∞–π–¥–∞–≥ (–∂–∏—à—ç—ç –Ω—å, —Ö—ç—Ä–≤—ç—ç “Ø–Ω—ç —Ç”©–≥—Ä”©–≥”©”©—Ä –±–æ–ª RMSE –º”©–Ω —Ç”©–≥—Ä”©–≥”©”©—Ä —Ö—ç–º–∂–∏–≥–¥—ç–Ω—ç).

# Chapter 2
Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways ==> 
from sklearn.model_selection import train_test_split
80% train, 20% test
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

Correlation is a statistical measure that shows how strongly two variables are related to each other.
If two things change together in a predictable way, they are said to be correlated.

The most common measure is the correlation coefficient (often denoted as r), which ranges from -1 to +1:
    +1 ‚Üí perfect positive correlation (when one increases, the other always increases).
    0 ‚Üí no correlation (no relationship between them).
    -1 ‚Üí perfect negative correlation (when one increases, the other always decreases)

Example:
    Positive correlation: The number of hours studied and exam score ‚Üí usually, more study hours means higher scores.
    Negative correlation: The speed of a car and the time it takes to reach a destination ‚Üí higher speed means less time.
    No correlation: Shoe size and intelligence ‚Üí unrelated.


You create these new attributes as follows:
    df_house["rooms_per_house"] = df_house["total_rooms"] / df_house["households"]
    df_house["bedrooms_ratio"] = df_house["total_bedrooms"] / df_house["total_rooms"]
    df_house["people_per_house"] = df_house["population"] / df_house["households"]


Clean the Data

option 1
    –î—É—Ç—É—É —É—Ç–≥–∞ –±“Ø—Ö–∏–π –º”©—Ä“Ø“Ø–¥–∏–π–≥ —É—Å—Ç–≥–∞–Ω–∞
    housing.dropna(subset=["total_bedrooms"], inplace=True) 

option 2
    –ë–∞–≥–∞–Ω—ã–≥ –±“Ø—Ö—ç–ª–¥ –Ω—å —É—Å—Ç–≥–∞–Ω–∞
    housing.drop("total_bedrooms", axis=1)  

option 3
    Fill missing values with the median
    median = housing["total_bedrooms"].median() 
    housing["total_bedrooms"].fillna(median, inplace=True)


You decide to go for option 3 since it is the least destructive

option 4
you will use a handy Scikit-Learn class: SimpleImputer.
The benefit is that it will store the median value of each feature:

imputer = SimpleImputer(strategy="median")
housing_num = df_house.select_dtypes(include=[np.number])
imputer.fit(housing_num)

Since the median can only be computed on numerical attributes, you thenneed to create a copy of the data with only the numerical attributes

The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable.
print(imputer.statistics_)

another option
print(df_house .median(numeric_only=True).values)

Now you can use this ‚Äútrained‚Äù imputer to transform the training set by replacing missing values with the learned medians:
X = imputer.transform(housing_num)

output of imputer.transform(housing_num) is a NumPy array: X has neither column names nor index. Luckily, it‚Äôs not too hard to wrap X in a DataFrame and recover the column names and index from housing_num:
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
index=housing_num.index)

TIP
There are also more powerful imputers available in the sklearn.impute package (both fornumerical features only):
KNNImputer replaces each missing value with the mean of the k-nearest neighbors‚Äôvalues for that feature. The distance is based on all the available features.


# Handling Text and Categorical Attributes  
Most machine learning algorithms prefer to work with numbers, so let‚Äôs convert these categories from text to numbers
we can use Scikit-Learn‚Äôs OrdinalEncoder class:
from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)

option 2
The new attributes are sometimes called dummy attributes.Scikit-Learn provides a OneHotEncoder class to convert categorical values
into one-hot vectors:

from sklearn.preprocessing import OneHotEncoder
# Handling Text and Categorical Attributes
data = pd.DataFrame({"ocean_proximity": ["NEAR BAY", "INLAND", "NEAR OCEAN", "INLAND"]})

cat_encoder = OneHotEncoder()
X = df_house[["ocean_proximity"]]
X_encoded = cat_encoder.fit_transform(X)

X_encoded_df = pd.DataFrame(
    X_encoded.toarray(),
    columns=cat_encoder.get_feature_names_out(["ocean_proximity"]),
    index=df_house.index  # now X_encoded has same number of rows as df_house
)

df_house = df_house.drop("ocean_proximity", axis=1)
df_house = pd.concat([df_house, X_encoded_df], axis=1)


By default, the output of a OneHotEncoder is a SciPy sparse matrix, instead of a NumPy array:
the advantage of OneHotEncoder is that it remembers which categories it wastrained on


# Feature Scaling and Transformation
# Feature Scaling
the numerical features in your dataset have been transformed to be on a similar scale.
Feature Scaling –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥—ã–Ω —É—Ç–≥—É—É–¥—ã–≥ –Ω—ç–≥ —Ö—ç–º–∂—ç—ç—Å —Ä“Ø“Ø –æ—Ä—É—É–ª–∞—Ö “Ø–π–ª —è–≤—Ü —é–º. –£—á–∏—Ä –Ω—å –∑–∞—Ä–∏–º –º–∞—à–∏–Ω —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º—É—É–¥ (gradient descent, k-NN, SVM, neural networks) ”©–≥”©–≥–¥–ª–∏–π–Ω —É—Ç–≥—ã–Ω —Ö—ç–º–∂—ç—ç—Å—ç—ç—Å —Ö–∞–º–∞–∞—Ä—á –∞–∂–∏–ª–ª–∞–¥–∞–≥.

machine learning algorithms don‚Äôt perform well when the input numerical attributes have very different scales.
    –ñ–∏—à—ç—ç:
        –ë–∞–π—à–∏–Ω–≥–∏–π–Ω —Ö—ç–º–∂—ç—ç (square feet): 0 ‚Äì 5000
        –£–Ω—Ç–ª–∞–≥—ã–Ω ”©—Ä”©”©–Ω–∏–π —Ç–æ–æ: 0 ‚Äì 5
Purpose: Prevent features with large values (e.g., income in thousands) from dominating smaller features (e.g., age).
There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.
    –ï—Ä”©–Ω—Ö–∏–π –∞—Ä–≥–∞:
        Min-Max(Normalization) Scaling (values between 0 and 1)
        Standardization –ë–∞–≥–∞–Ω—ã–≥ mean = 0, std = 1 –±–æ–ª–≥–æ–Ω–æ.
        Max Abs Scaling (Sparse ”©–≥”©–≥–¥”©–ª–¥ —Ç–æ—Ö–∏—Ä–æ–º–∂—Ç–æ–π)
    –ë–∞–≥–∞–Ω—ã–Ω —É—Ç–≥—ã–≥ [-1,1] —Ö“Ø—Ä—Ç—ç–ª –∂–∏–≥–¥–ª—ç–Ω—ç.

    Min-max scaling:
        from sklearn.preprocessing import MinMaxScaler
        min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
        housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
    Standardization scaling:
        std_scaler = StandardScaler()
        housing_num_std_scaled = std_scaler.fit_transform(housing_num)

# Capped data
means extreme values (outliers) have been limited to a certain threshold.(–•—ç—Ç—ç—Ä—Ö–∏–π —Ö—ç—Ç—ç—Ä—Ö–∏–π –∏—Ö/–±–∞–≥–∞ —É—Ç–≥—É—É–¥—ã–≥ (outliers) —Ç–æ–¥–æ—Ä—Ö–æ–π –±–æ—Å–≥–æ–Ω–¥ —Ö—è–∑–≥–∞–∞—Ä–ª–∞—Å–∞–Ω.)

    Example: If most incomes are below $200k but some are $5M, you might cap all values above $200k at $200k.
    –ñ–∏—à—ç—ç: –•—ç—Ä—ç–≤ –∏—Ö—ç–Ω—Ö –æ—Ä–ª–æ–≥–æ 200,000$-–æ–æ—Å –¥–æ–æ—à –±–∞–π—Ö–∞–¥ –∑–∞—Ä–∏–º –Ω—å 5,000,000$ –±–æ–ª 200,000$-–æ–æ—Å –¥—ç—ç—à —É—Ç–≥—ã–≥ –±“Ø–≥–¥–∏–π–≥ –Ω—å 200,000 –≥—ç–∂ –∞–≤–¥–∞–≥.
    Purpose: Reduce the negative impact of outliers on model training.
    –ó–æ—Ä–∏–ª–≥–æ: Outlier-—É—É–¥ –∑–∞–≥–≤–∞—Ä—ã–≥ –±—É—Ä—É—É —Å—É—Ä–≥–∞—Ö–∞–∞—Å —Å—ç—Ä–≥–∏–π–ª—ç—Ö.

# clustering
Clustering is the process of grouping similar data points together based on their features, without using labeled data.
When data is clustered, it means the algorithm has divided the dataset into groups (called clusters) such that:

    Points in the same cluster are more similar to each other.
    Points in different clusters are more different from each other.

Example:
    Suppose you have data about customers:
        Age
        Income
        Spending habits

    A clustering algorithm might group them into:
        Cluster 1: Young, low income, low spending

        Cluster 2: Middle-aged, medium income, medium spending

        Cluster 3: Older, high income, high spending
        
No labels are provided‚Äîthe algorithm ‚Äúdiscovers‚Äù these groups automatically.

# Stratified
In machine learning, ‚Äústratified‚Äù usually refers to stratified sampling or stratified splitting, which is a way of dividing data into train/test (or folds in cross-validation) while preserving the distribution of labels or classes.

üìä Example:
Suppose you have a dataset with 100 samples:
    80 are class A

    20 are class B

If you do a random split into 80% training and 20% testing, you might end up with:
    Train: 70 A, 10 B

    Test: 10 A, 10 B (imbalanced compared to original)

But with a stratified split, the ratio is preserved:
    Train: 64 A, 16 B (still 80:20 ratio)

    Test: 16 A, 4 B (still 80:20 ratio)

# Why it‚Äôs important:
    Prevents bias in model evaluation.
    Especially useful for imbalanced datasets (where some classes are rare).
    Helps the model generalize better and ensures fair testing.

# Feature transformations
In Machine Learning (ML), a transformation means applying some operation to your data (inputs, features, or sometimes outputs) to make it more suitable for training a model or improving performance

Feature Transformation –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–Ω –±–∞–≥–∞–Ω—ã–Ω —É—Ç–≥—ã–≥ —à–∏–Ω—ç —Ö—ç–ª–±—ç—Ä—Ç –æ—Ä—É—É–ª–∞—Ö “Ø–π–ª —è–≤—Ü —é–º.
    –Ø–∞–≥–∞–∞–¥ —Ö—ç—Ä—ç–≥—Ç—ç–π –≤—ç:
        ”®–≥”©–≥–¥–ª–∏–π–Ω —Ç–∞—Ä—Ö–∞–ª—Ç—ã–≥ –Ω–æ—Ä–º–∞–ª–¥ –æ–π—Ä—Ç—É—É–ª–∂, —Å—É—Ä–≥–∞–ª—Ç—ã–Ω –∞–ª–≥–æ—Ä–∏—Ç–º—ã–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö
        –®—É–≥–∞–º–∞–Ω –±—É—Å (non-linear) —Ö–∞–º–∞–∞—Ä–ª—ã–≥ –∑–∞—Å–∞—Ö
    –ï—Ä”©–Ω—Ö–∏–π –∞—Ä–≥–∞
    Log Transformation
    –ë–∞—Ä—É—É–Ω —Ç–∏–π—à —Ö–∞–∑–∞–π—Å–∞–Ω ”©–≥”©–≥–¥–ª–∏–π–≥ –∂–∏–≥–¥–ª—ç—Ö:
        X_log = np.log1p(X)
    Power / Box-Cox Transformation
    ”®–≥”©–≥–¥–ª–∏–π–≥ Gaussian —Ç–∞—Ä—Ö–∞–ª—Ç—Ç–∞–π –æ–π—Ä—Ç—É—É–ª–∞—Ö

    from sklearn.preprocessing import PowerTransformer
    pt = PowerTransformer()
    X_transformed = pt.fit_transform(X)

Polynomial Features
Interaction –±—É—é—É –∫–≤–∞–¥—Ä–∞—Ç, –∫—É–± –∑—ç—Ä—ç–≥ ”©–Ω–¥”©—Ä –∑—ç—Ä—ç–≥–ª—ç–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥ –Ω—ç–º—ç—Ö
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

    üí° –î“Ø–≥–Ω—ç–ª—Ç:
        Scaling ‚Üí –ê–ª–≥–æ—Ä–∏—Ç–º—É—É–¥—ã–≥ –∑”©–≤ –∞–∂–∏–ª–ª—É—É–ª–∞—Ö–∞–¥ —á—É—Ö–∞–ª
        Transformation ‚Üí model –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Å–∞–π–∂—Ä—É—É–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞–≥–¥–¥–∞–≥

the values are shifted
1Ô∏è‚É£ Mean shift (–¥—É–Ω–¥–∞–∂ —à–∏–ª–∂“Ø“Ø–ª—ç–ª—Ç)
–ú—ç–¥—ç—ç–ª–ª–∏–π–Ω –±–∞–≥–∞–Ω—É—É–¥—ã–Ω –¥—É–Ω–¥–∞–∂ (mean) –Ω—å 0-–¥ –æ–π—Ä –±–∞–π—Ö–∞–∞—Ä shift —Ö–∏–π–≥–¥–¥—ç–≥.
    –ñ–∏—à—ç—ç:
        Original feature: [2, 4, 6, 8]
    Mean = 5
        Shifted (centered) = [2-5, 4-5, 6-5, 8-5] = [-3, -1, 1, 3]  
“Æ“Ø–Ω–∏–π–≥ Mean normalization –≥—ç–∂ –Ω—ç—Ä–ª—ç–¥—ç–≥.
    –£—á–∏—Ä –Ω—å:
        ML –∞–ª–≥–æ—Ä–∏—Ç–º—É—É–¥ (–∂–∏—à—ç—ç –Ω—å: gradient descent, PCA) –Ω—å centered data-–¥ —Ö—É—Ä–¥–∞–Ω –∞–∂–∏–ª–ª–∞–¥–∞–≥.


# what is heavy tail 
In Machine Learning (ML) and statistics, a heavy tail refers to a probability distribution where the probability of very large values (extreme outcomes) decreases more slowly than it would in a "normal" distribution like the Gaussian.—ç?

Heavy tail –≥—ç–¥—ç–≥ –Ω—å ”©–≥”©–≥–¥–ª–∏–π–Ω –º–∞–≥–∞–¥–ª–∞–ª—ã–Ω —Ç–∞—Ä—Ö–∞–ª—Ç –Ω—å –º–∞—à –∏—Ö —É—Ç–≥—É—É–¥ (—ç–∫—Å—Ç—Ä–µ–º —É—Ç–≥—É—É–¥) –≥–∞—Ä–∞—Ö –º–∞–≥–∞–¥–ª–∞–ª –∏—Ö –±–∞–π–¥–∞–≥ –≥—ç—Å—ç–Ω “Ø–≥ —é–º.

”®”©—Ä”©”©—Ä —Ö—ç–ª–±—ç–ª:
–ï—Ä–¥–∏–π–Ω (Normal) —Ç–∞—Ä—Ö–∞–ª—Ç ‚Üí –¥—É–Ω–¥–∞–∂–∏–π–Ω –æ—Ä—á–∏–º–¥ —É—Ç–≥—É—É–¥ –∏—Ö —Ç”©–≤–ª”©—Ä—á, –º–∞—à —Ç–æ–º —ç—Å–≤—ç–ª –º–∞—à –∂–∏–∂–∏–≥ —É—Ç–≥—É—É–¥ —Ö–æ–≤–æ—Ä —Ç–æ—Ö–∏–æ–ª–¥–æ–Ω–æ.
Heavy-tailed —Ç–∞—Ä—Ö–∞–ª—Ç (–∂–∏—à—ç—ç –Ω—å: Pareto, Cauchy, Student‚Äôs t) ‚Üí –º–∞—à —Ç–æ–º —É—Ç–≥—É—É–¥, –æ–Ω—Ü–≥–æ–π (outlier) —É—Ç–≥—É—É–¥ –∏–ª“Ø“Ø –æ–ª–æ–Ω –≥–∞—Ä–¥–∞–≥.

–ñ–∏—à—ç—ç:
    –•“Ø–Ω –∞–º—ã–Ω –æ—Ä–ª–æ–≥–æ: –∏—Ö—ç–Ω—Ö –Ω—å –¥—É–Ω–¥–∞–∂ –æ—Ä—á–∏–º–¥, –≥—ç—Ö–¥—ç—ç –º–∞—à —Ü”©”©–Ω —Ö—ç–¥ –Ω—å –º–∞—à ”©–Ω–¥”©—Ä –æ—Ä–ª–æ–≥–æ—Ç–æ–π –±–∞–π–¥–∞–≥.
    –ò–Ω—Ç–µ—Ä–Ω—ç—Ç–∏–π–Ω —Å“Ø–ª–∂—ç—ç–Ω–∏–π –∞—á–∞–∞–ª–∞–ª: –∏—Ö—ç–Ω—Ö “Ø–µ–¥ —Ö—ç–≤–∏–π–Ω, –≥—ç—Ö–¥—ç—ç —Ö–∞–∞—è–∞ –º–∞—à –∏—Ö ”©—Å”©–ª—Ç—Ç—ç–π ‚Äúspike‚Äù –≥–∞—Ä–¥–∞–≥.
    –≠–Ω—ç –±–æ–ª heavy tail “Ø–∑—ç–≥–¥—ç–ª.